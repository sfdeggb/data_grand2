{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.feature_column import SparseFeat, DenseFeat, VarLenSparseFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: deepctr\n",
      "Version: 0.9.3\n",
      "Summary: Easy-to-use,Modular and Extendible package of deep learning based CTR(Click Through Rate) prediction models with tensorflow 1.x and 2.x .\n",
      "Home-page: https://github.com/shenweichen/deepctr\n",
      "Author: Weichen Shen\n",
      "Author-email: weichenswc@163.com\n",
      "License: Apache-2.0\n",
      "Location: e:\\aconada\\envs\\env106\\lib\\site-packages\n",
      "Requires: h5py, requests\n",
      "Required-by: \n",
      "Name: tensorflow\n",
      "Version: 2.8.4\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: e:\\aconada\\envs\\env106\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show deepctr\n",
    "! pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "    \"\"\"构建特征列\"\"\"\n",
    "    # 稀疏特征\n",
    "    sparse_features = [\n",
    "        SparseFeat('user_id', vocabulary_size=100, embedding_dim=8),\n",
    "        SparseFeat('gender', vocabulary_size=2, embedding_dim=4),\n",
    "        SparseFeat('region', vocabulary_size=5, embedding_dim=4),\n",
    "        SparseFeat('user_hot', vocabulary_size=3, embedding_dim=4),\n",
    "        SparseFeat('video_id', vocabulary_size=100, embedding_dim=8),\n",
    "        SparseFeat('video_type', vocabulary_size=5, embedding_dim=4),\n",
    "        SparseFeat('video_quality', vocabulary_size=2, embedding_dim=4)\n",
    "    ]\n",
    "    \n",
    "    # 稠密特征\n",
    "    dense_features = [\n",
    "        DenseFeat('age', 1),\n",
    "        DenseFeat('video_length', 1)\n",
    "    ]\n",
    "    \n",
    "    # 序列特征\n",
    "    seq_features = [\n",
    "        VarLenSparseFeat(\n",
    "            SparseFeat('hist_video_id', vocabulary_size=100, embedding_dim=8, embedding_name='video_id'),\n",
    "            maxlen=50,\n",
    "            length_name=\"seq_length\"\n",
    "        ),\n",
    "        VarLenSparseFeat(\n",
    "            SparseFeat('hist_video_type', vocabulary_size=5, embedding_dim=4, embedding_name='video_type'),\n",
    "            maxlen=50,\n",
    "            length_name=\"seq_length\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return sparse_features, dense_features, seq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.inputs import embedding_lookup \n",
    "from deepctr.feature_column import build_input_features,input_from_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.layers.utils import combined_dnn_input\n",
    "from deepctr.layers.sequence import AttentionSequencePoolingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.layers.core import DNN\n",
    "from deepctr.layers.core import PredictionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIN_MMOE(\n",
    "    dnn_feature_columns,\n",
    "    history_feature_list,\n",
    "    num_experts=4,\n",
    "    expert_dnn_hidden_units=(256, 128),\n",
    "    tower_dnn_hidden_units=(64,),\n",
    "    gate_dnn_hidden_units=(),\n",
    "    l2_reg_embedding=1e-6,\n",
    "    l2_reg_dnn=0,\n",
    "    seed=1024,\n",
    "    dnn_dropout=0,\n",
    "    dnn_activation='relu',\n",
    "    dnn_use_bn=False,\n",
    "    task_types=('binary', 'regression', 'binary', 'binary'),\n",
    "    task_names=('is_exceed_5s', 'stay_time', 'is_watch', 'is_buy')\n",
    "):\n",
    "    \"\"\"\n",
    "    DIN和MMOE结合的多任务学习模型\n",
    "    \n",
    "    参数:\n",
    "        dnn_feature_columns: 特征列配置\n",
    "        history_feature_list: 历史行为特征列表\n",
    "        num_experts: 专家网络数量\n",
    "        expert_dnn_hidden_units: 专家网络隐藏层配置\n",
    "        tower_dnn_hidden_units: 任务塔网络配置\n",
    "        gate_dnn_hidden_units: 门控网络配置\n",
    "        ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # 构建输入层\n",
    "    features = build_input_features(dnn_feature_columns)\n",
    "    inputs_list = list(features.values())\n",
    "    \n",
    "    # 处理特征embedding\n",
    "    sparse_embedding_list, dense_value_list = input_from_feature_columns(\n",
    "        features, dnn_feature_columns, l2_reg_embedding, seed\n",
    "    )\n",
    "    \n",
    "    history_feature_columns = []\n",
    "    for feat_name in history_feature_list:\n",
    "        for feat in sparse_embedding_list:\n",
    "            if feat.name == feat_name:\n",
    "                history_feature_columns.append(feat)\n",
    "                break\n",
    "    # 获取目标item的embedding\n",
    "    query_embed_list = embedding_lookup(sparse_embedding_list, features, history_feature_columns)\n",
    "    keys_embed_list = embedding_lookup(sparse_embedding_list, features, history_feature_columns, history_feature_columns)\n",
    "    \n",
    "    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "\n",
    "    # DIN注意力层处理\n",
    "    att_output = AttentionSequencePoolingLayer()(\n",
    "        [query_embed_list, keys_embed_list, features['seq_length']]\n",
    "    )\n",
    "    \n",
    "    # 合并DIN输出和其他特征\n",
    "    din_output = tf.concat([dnn_input, att_output], axis=-1)\n",
    "    # 构建专家网络\n",
    "    expert_outputs = []\n",
    "    for i in range(num_experts):\n",
    "        expert_network = DNN(\n",
    "            expert_dnn_hidden_units,\n",
    "            dnn_activation,\n",
    "            l2_reg_dnn,\n",
    "            dnn_dropout,\n",
    "            dnn_use_bn,\n",
    "            seed=seed,\n",
    "            name=f'expert_{i}'\n",
    "        )(din_output)\n",
    "        expert_outputs.append(expert_network)\n",
    "    \n",
    "    expert_concat = tf.stack(expert_outputs, axis=1)  # (batch_size, num_experts, expert_output_dim)\n",
    "    \n",
    "    # 构建每个任务的门控网络和输出\n",
    "    task_outputs = []\n",
    "    for i, (task_type, task_name) in enumerate(zip(task_types, task_names)):\n",
    "        # 门控网络\n",
    "        gate_input = DNN(\n",
    "            gate_dnn_hidden_units,\n",
    "            dnn_activation,\n",
    "            l2_reg_dnn,\n",
    "            dnn_dropout,\n",
    "            dnn_use_bn,\n",
    "            seed=seed,\n",
    "            name=f'gate_{task_name}'\n",
    "        )(din_output)\n",
    "        \n",
    "        gate_output = Dense(\n",
    "            num_experts,\n",
    "            use_bias=False,\n",
    "            activation='softmax',\n",
    "            name=f'gate_softmax_{task_name}'\n",
    "        )(gate_input)\n",
    "        \n",
    "        gate_output = tf.expand_dims(gate_output, axis=-1)\n",
    "        \n",
    "        # 专家组合\n",
    "        weighted_expert = reduce_sum(\n",
    "            expert_concat * gate_output,\n",
    "            axis=1,\n",
    "            keep_dims=False,\n",
    "            name=f'gate_mul_expert_{task_name}'\n",
    "        )\n",
    "        \n",
    "        # 任务塔网络\n",
    "        tower_output = DNN(\n",
    "            tower_dnn_hidden_units,\n",
    "            dnn_activation,\n",
    "            l2_reg_dnn,\n",
    "            dnn_dropout,\n",
    "            dnn_use_bn,\n",
    "            seed=seed,\n",
    "            name=f'tower_{task_name}'\n",
    "        )(weighted_expert)\n",
    "        \n",
    "        # 输出层\n",
    "        logit = Dense(1, use_bias=False)(tower_output)\n",
    "        output = PredictionLayer(task_type, name=task_name)(logit)\n",
    "        task_outputs.append(output)\n",
    "    \n",
    "    # 构建模型\n",
    "    model = Model(inputs=inputs_list, outputs=task_outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected inputs dimensions,the 3 tensor dimensions are 0,0 and 2 , expect to be 3,3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m dnn_feature_columns \u001b[38;5;241m=\u001b[39m sparse_features \u001b[38;5;241m+\u001b[39m dense_features \u001b[38;5;241m+\u001b[39m seq_features\n\u001b[0;32m      6\u001b[0m history_feature_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDIN_MMOE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdnn_feature_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_feature_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_experts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_dnn_hidden_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtower_dnn_hidden_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_exceed_5s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstay_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_watch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_buy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(model, to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./DIN_MMOE.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mDIN_MMOE\u001b[1;34m(dnn_feature_columns, history_feature_list, num_experts, expert_dnn_hidden_units, tower_dnn_hidden_units, gate_dnn_hidden_units, l2_reg_embedding, l2_reg_dnn, seed, dnn_dropout, dnn_activation, dnn_use_bn, task_types, task_names)\u001b[0m\n\u001b[0;32m     49\u001b[0m dnn_input \u001b[38;5;241m=\u001b[39m combined_dnn_input(sparse_embedding_list, dense_value_list)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# DIN注意力层处理\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m att_output \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionSequencePoolingLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_embed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_embed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseq_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 合并DIN输出和其他特征\u001b[39;00m\n\u001b[0;32m     57\u001b[0m din_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([dnn_input, att_output], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32me:\\aconada\\envs\\env106\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:983\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 983\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32me:\\aconada\\envs\\env106\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1121\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1119\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[0;32m   1120\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1121\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1126\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1127\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\aconada\\envs\\env106\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:854\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 854\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\aconada\\envs\\env106\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:892\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_scope()):  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    888\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;66;03m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# overridden).\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m    894\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\aconada\\envs\\env106\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2654\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_default\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   2650\u001b[0m   \u001b[38;5;66;03m# Any setup work performed only once should happen in an `init_scope`\u001b[39;00m\n\u001b[0;32m   2651\u001b[0m   \u001b[38;5;66;03m# to avoid creating symbolic Tensors that will later pollute any eager\u001b[39;00m\n\u001b[0;32m   2652\u001b[0m   \u001b[38;5;66;03m# operations.\u001b[39;00m\n\u001b[0;32m   2653\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39mmaybe_init_scope(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint:disable=not-callable\u001b[39;00m\n\u001b[0;32m   2655\u001b[0m \u001b[38;5;66;03m# We must set also ensure that the layer is marked as built, and the build\u001b[39;00m\n\u001b[0;32m   2656\u001b[0m \u001b[38;5;66;03m# shape is stored since user defined build functions may not be calling\u001b[39;00m\n\u001b[0;32m   2657\u001b[0m \u001b[38;5;66;03m# `super.build()`\u001b[39;00m\n\u001b[0;32m   2658\u001b[0m Layer\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m, input_shapes)\n",
      "File \u001b[1;32me:\\aconada\\envs\\env106\\lib\\site-packages\\deepctr\\layers\\sequence.py:236\u001b[0m, in \u001b[0;36mAttentionSequencePoolingLayer.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA `AttentionSequencePoolingLayer` layer should be called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    233\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon a list of 3 inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_shape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_shape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_shape[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected inputs dimensions,the 3 tensor dimensions are \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m , expect to be 3,3 and 2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;28mlen\u001b[39m(input_shape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(input_shape[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(input_shape[\u001b[38;5;241m2\u001b[39m])))\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m input_shape[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m input_shape[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m input_shape[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA `AttentionSequencePoolingLayer` layer requires \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    242\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs of a 3 tensor with shape (None,1,embedding_size),(None,T,embedding_size) and (None,1)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    243\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGot different shapes: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (input_shape))\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected inputs dimensions,the 3 tensor dimensions are 0,0 and 2 , expect to be 3,3 and 2"
     ]
    }
   ],
   "source": [
    "## 查看模型结构\n",
    "from tensorflow import keras\n",
    "\n",
    "sparse_features, dense_features, seq_features = get_feature_columns()\n",
    "dnn_feature_columns = sparse_features + dense_features + seq_features\n",
    "history_feature_list = ['video_id', 'video_type']\n",
    "\n",
    "model = DIN_MMOE(\n",
    "    dnn_feature_columns,\n",
    "    history_feature_list,\n",
    "    num_experts=4,\n",
    "    expert_dnn_hidden_units=(256, 128),\n",
    "    tower_dnn_hidden_units=(64,),\n",
    "    task_types=('binary', 'regression', 'binary', 'binary'),\n",
    "    task_names=('is_exceed_5s', 'stay_time', 'is_watch', 'is_buy')\n",
    ")\n",
    "keras.utils.plot_model(model, to_file=\"./DIN_MMOE.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (957024180.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def DIN_MMOE(...):\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def DIN_MMOE(...):\n",
    "    # ... existing code ...\n",
    "    \n",
    "    # 修改这部分代码\n",
    "    # 从特征列中找到对应的SparseFeat对象\n",
    "    history_feature_columns = []\n",
    "    for feat_name in history_feature_list:\n",
    "        for feat in sparse_embedding_list:\n",
    "            if feat.name == feat_name:\n",
    "                history_feature_columns.append(feat)\n",
    "                break\n",
    "    \n",
    "    # 使用特征列对象而不是特征名称\n",
    "    query_embed_list = embedding_lookup(sparse_embedding_list, features, history_feature_columns)\n",
    "    keys_embed_list = embedding_lookup(sparse_embedding_list, features, history_feature_columns, history_feature_columns)\n",
    "    \n",
    "    # ... rest of the code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # 获取特征列配置\n",
    "    sparse_features, dense_features, seq_features = get_feature_columns()\n",
    "    dnn_feature_columns = sparse_features + dense_features + seq_features\n",
    "    history_feature_list = ['video_id', 'video_type']\n",
    "    \n",
    "    # 构建模型\n",
    "    model = DIN_MMOE(\n",
    "        dnn_feature_columns,\n",
    "        history_feature_list,\n",
    "        num_experts=4,\n",
    "        expert_dnn_hidden_units=(256, 128),\n",
    "        tower_dnn_hidden_units=(64,),\n",
    "        task_types=('binary', 'regression', 'binary', 'binary'),\n",
    "        task_names=('is_exceed_5s', 'stay_time', 'is_watch', 'is_buy')\n",
    "    )\n",
    "    \n",
    "    # 编译模型\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss={\n",
    "            'is_exceed_5s': 'binary_crossentropy',\n",
    "            'stay_time': 'mse',\n",
    "            'is_watch': 'binary_crossentropy',\n",
    "            'is_buy': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'is_exceed_5s': 1.0,\n",
    "            'stay_time': 1.0,\n",
    "            'is_watch': 1.0,\n",
    "            'is_buy': 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            'is_exceed_5s': ['AUC'],\n",
    "            'stay_time': ['mse'],\n",
    "            'is_watch': ['AUC'],\n",
    "            'is_buy': ['AUC']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(\n",
    "        train_model_input,\n",
    "        [train['is_exceed_5s'], train['stay_time'], \n",
    "         train['is_watch'], train['is_buy']],\n",
    "        batch_size=256,\n",
    "        epochs=10,\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.inputs import create_embedding_matrix,get_dense_input,varlen_embedding_lookup,get_varlen_pooling_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.layers.utils import concat_func,reduce_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Dense, Flatten,Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIN_MMOE_2(\n",
    "    dnn_feature_columns,\n",
    "    history_feature_list,\n",
    "    num_experts=4,\n",
    "    expert_dnn_hidden_units=(256, 128),\n",
    "    tower_dnn_hidden_units=(64,),\n",
    "    gate_dnn_hidden_units=(),\n",
    "    l2_reg_embedding=1e-6,\n",
    "    l2_reg_dnn=0,\n",
    "    seed=1024,\n",
    "    dnn_dropout=0,\n",
    "    dnn_activation='relu',\n",
    "    dnn_use_bn=False,\n",
    "    num_tasks =4,\n",
    "    task_types=('binary', 'regression', 'binary', 'binary'),\n",
    "    task_names=('is_exceed_5s', 'stay_time', 'is_watch', 'is_buy')\n",
    "):\n",
    "    \"\"\"\n",
    "    DIN和MMOE结合的多任务学习模型\n",
    "    \n",
    "    参数:\n",
    "        dnn_feature_columns: 特征列配置\n",
    "        history_feature_list: 历史行为特征列表\n",
    "        num_experts: 专家网络数量\n",
    "        expert_dnn_hidden_units: 专家网络隐藏层配置\n",
    "        tower_dnn_hidden_units: 任务塔网络配置\n",
    "        gate_dnn_hidden_units: 门控网络配置\n",
    "        ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # 构建输入层\n",
    "    features = build_input_features(dnn_feature_columns)\n",
    "    \n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "    dense_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "    \n",
    "    history_feature_columns = []\n",
    "    sparse_varlen_feature_columns = []\n",
    "    history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        if feature_name in history_fc_names:\n",
    "            history_feature_columns.append(fc)\n",
    "        else:\n",
    "            sparse_varlen_feature_columns.append(fc)\n",
    "\n",
    "    inputs_list = list(features.values())\n",
    "\n",
    "    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, seed, prefix=\"\")\n",
    "\n",
    "    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns, history_feature_list,\n",
    "                                        history_feature_list, to_list=True)\n",
    "    keys_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns, history_fc_names,\n",
    "                                        history_fc_names, to_list=True)\n",
    "    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,\n",
    "                                            mask_feat_list=history_feature_list, to_list=True)\n",
    "    dense_value_list = get_dense_input(features, dense_feature_columns)\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict, features, sparse_varlen_feature_columns)\n",
    "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,\n",
    "                                                    to_list=True)\n",
    "\n",
    "    dnn_input_emb_list += sequence_embed_list\n",
    "\n",
    "    keys_emb = concat_func(keys_emb_list, mask=True)\n",
    "    deep_input_emb = concat_func(dnn_input_emb_list)\n",
    "    query_emb = concat_func(query_emb_list, mask=True)\n",
    "    hist = AttentionSequencePoolingLayer(supports_masking=True)([\n",
    "        query_emb, keys_emb])\n",
    "    \n",
    "    \n",
    "    deep_input_emb = concat_func([deep_input_emb, hist])\n",
    "    deep_input_emb = Flatten()(deep_input_emb)\n",
    "    din_output = combined_dnn_input([deep_input_emb], dense_value_list)\n",
    "    \n",
    "    \n",
    "    # build expert layer\n",
    "    expert_outs = []\n",
    "    for i in range(num_experts):\n",
    "        expert_network = DNN(expert_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,\n",
    "                             name='expert_' + str(i))(din_output)\n",
    "        expert_outs.append(expert_network)\n",
    "\n",
    "    expert_concat = Lambda(lambda x: tf.stack(x, axis=1))(expert_outs)  # None,num_experts,dim\n",
    "\n",
    "    mmoe_outs = []\n",
    "    for i in range(num_tasks):  # one mmoe layer: nums_tasks = num_gates\n",
    "        # build gate layers\n",
    "        gate_input = DNN(gate_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,\n",
    "                         name='gate_' + task_names[i])(din_output)\n",
    "        gate_out = Dense(num_experts, use_bias=False, activation='softmax',\n",
    "                         name='gate_softmax_' + task_names[i])(gate_input)\n",
    "        gate_out = Lambda(lambda x: tf.expand_dims(x, axis=-1))(gate_out)\n",
    "\n",
    "        # gate multiply the expert\n",
    "        gate_mul_expert = Lambda(lambda x: reduce_sum(x[0] * x[1], axis=1, keep_dims=False),\n",
    "                                 name='gate_mul_expert_' + task_names[i])([expert_concat, gate_out])\n",
    "        mmoe_outs.append(gate_mul_expert)\n",
    "\n",
    "    task_outs = []\n",
    "    for task_type, task_name, mmoe_out in zip(task_types, task_names, mmoe_outs):\n",
    "        # build tower layer\n",
    "        tower_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,\n",
    "                           name='tower_' + task_name)(mmoe_out)\n",
    "\n",
    "        logit = Dense(1, use_bias=False)(tower_output)\n",
    "        output = PredictionLayer(task_type, name=task_name)(logit)\n",
    "        task_outs.append(output)\n",
    "\n",
    "    model = Model(inputs=inputs_list, outputs=task_outs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "## 查看模型结构\n",
    "from tensorflow import keras\n",
    "\n",
    "sparse_features, dense_features, seq_features = get_feature_columns()\n",
    "dnn_feature_columns = sparse_features + dense_features + seq_features\n",
    "history_feature_list = ['video_id', 'video_type']\n",
    "\n",
    "model = DIN_MMOE_2(\n",
    "    dnn_feature_columns,\n",
    "    history_feature_list,\n",
    "    num_experts=4,\n",
    "    expert_dnn_hidden_units=(256, 128),\n",
    "    tower_dnn_hidden_units=(64,),\n",
    "    task_types=('binary', 'regression', 'binary', 'binary'),\n",
    "    task_names=('is_exceed_5s', 'stay_time', 'is_watch', 'is_buy')\n",
    ")\n",
    "keras.utils.plot_model(model, to_file=\"DIN_MMOE.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1538279713.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    opt/homebrew/Cellar/graphviz/12.2.1/bin()\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "/opt/homebrew/Cellar/graphviz/12.2.1/bin\n",
    "\n",
    "export PATH=\"/opt/homebrew/Cellar/graphviz/12.2.1/bin:$PATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datagrand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
