{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y8dE8u_dXQJa",
        "outputId": "f8554884-59eb-44c8-f9ce-1f847a3568a3"
      },
      "outputs": [],
      "source": [
        "# 安装deepcrt库\n",
        "! pip install deepctr[cpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKo3DaFQm5xO",
        "outputId": "797a9149-c072-41ca-d788-8f3322fba49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.12.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, jax, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n"
          ]
        }
      ],
      "source": [
        "! pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi7uEDoGWqWQ",
        "outputId": "0f1b69d4-d468-439f-be73-b55fbf243f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.68.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.7.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.0)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.3.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VlW2FldW0xV",
        "outputId": "d812425e-f108-4a3c-f732-d3e38e54afe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.12.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, jax, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n"
          ]
        }
      ],
      "source": [
        "! pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAkeVBrWW_lv",
        "outputId": "da14de1e-a8db-45b6-e86f-ee364f0672f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: deepctr[cpu]\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip show deepctr[cpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dShGM3RjRWI"
      },
      "outputs": [],
      "source": [
        "## 工具类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "papvUE4LdjWI"
      },
      "outputs": [],
      "source": [
        "def reduce_mean(input_tensor,\n",
        "                axis=None,\n",
        "                keep_dims=False,\n",
        "                name=None,\n",
        "                reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_mean(input_tensor,\n",
        "                              axis=axis,\n",
        "                              keep_dims=keep_dims,\n",
        "                              name=name,\n",
        "                              reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_mean(input_tensor,\n",
        "                              axis=axis,\n",
        "                              keepdims=keep_dims,\n",
        "                              name=name)\n",
        "def reduce_sum(input_tensor,\n",
        "               axis=None,\n",
        "               keep_dims=False,\n",
        "               name=None,\n",
        "               reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor,\n",
        "                             axis=axis,\n",
        "                             keep_dims=keep_dims,\n",
        "                             name=name,\n",
        "                             reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_sum(input_tensor,\n",
        "                             axis=axis,\n",
        "                             keepdims=keep_dims,\n",
        "                             name=name)\n",
        "\n",
        "def reduce_max(input_tensor,\n",
        "               axis=None,\n",
        "               keep_dims=False,\n",
        "               name=None,\n",
        "               reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_max(input_tensor,\n",
        "                             axis=axis,\n",
        "                             keep_dims=keep_dims,\n",
        "                             name=name,\n",
        "                             reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_max(input_tensor,\n",
        "                             axis=axis,\n",
        "                             keepdims=keep_dims,name=name)\n",
        "\n",
        "def div(x, y, name=None):\n",
        "    try:\n",
        "        return tf.div(x, y, name=name)\n",
        "    except AttributeError:\n",
        "        return tf.divide(x, y, name=name)\n",
        "\n",
        "\n",
        "def softmax(logits, dim=-1, name=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, dim=dim, name=name)\n",
        "    except TypeError:\n",
        "        return tf.nn.softmax(logits, axis=dim, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn8zvlzrTWOt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Layer, Dropout\n",
        "class LocalActivationUnit(Layer):\n",
        "    \"\"\"The LocalActivationUnit used in DIN with which the representation of\n",
        "    user interests varies adaptively given different candidate items.\n",
        "\n",
        "      Input shape\n",
        "        - A list of two 3D tensor with shape:  ``(batch_size, 1, embedding_size)`` and ``(batch_size, T, embedding_size)``\n",
        "\n",
        "      Output shape\n",
        "        - 3D tensor with shape: ``(batch_size, T, 1)``.\n",
        "\n",
        "      Arguments\n",
        "        - **hidden_units**:list of positive integer, the attention net layer number and units in each layer.\n",
        "\n",
        "        - **activation**: Activation function to use in attention net.\n",
        "\n",
        "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix of attention net.\n",
        "\n",
        "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout in attention net.\n",
        "\n",
        "        - **use_bn**: bool. Whether use BatchNormalization before activation or not in attention net.\n",
        "\n",
        "        - **seed**: A Python integer to use as random seed.\n",
        "\n",
        "      References\n",
        "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_units=(64, 32), activation='sigmoid', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024,\n",
        "                 **kwargs):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.l2_reg = l2_reg\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_bn = use_bn\n",
        "        self.seed = seed\n",
        "        super(LocalActivationUnit, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
        "            raise ValueError('A `LocalActivationUnit` layer should be called '\n",
        "                             'on a list of 2 inputs')\n",
        "\n",
        "        if len(input_shape[0]) != 3 or len(input_shape[1]) != 3:\n",
        "            raise ValueError(\"Unexpected inputs dimensions %d and %d, expect to be 3 dimensions\" % (\n",
        "                len(input_shape[0]), len(input_shape[1])))\n",
        "\n",
        "        if input_shape[0][-1] != input_shape[1][-1] or input_shape[0][1] != 1:\n",
        "            raise ValueError('A `LocalActivationUnit` layer requires '\n",
        "                             'inputs of a two inputs with shape (None,1,embedding_size) and (None,T,embedding_size)'\n",
        "                             'Got different shapes: %s,%s' % (input_shape[0], input_shape[1]))\n",
        "        size = 4 * \\\n",
        "               int(input_shape[0][-1]\n",
        "                   ) if len(self.hidden_units) == 0 else self.hidden_units[-1]\n",
        "        self.kernel = self.add_weight(shape=(size, 1),\n",
        "                                      initializer=glorot_normal(\n",
        "                                          seed=self.seed),\n",
        "                                      name=\"kernel\")\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(1,), initializer=Zeros(), name=\"bias\")\n",
        "        self.dnn = DNN(self.hidden_units, self.activation, self.l2_reg, self.dropout_rate, self.use_bn, seed=self.seed)\n",
        "\n",
        "        super(LocalActivationUnit, self).build(\n",
        "            input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "\n",
        "        query, keys = inputs\n",
        "\n",
        "        keys_len = keys.get_shape()[1]\n",
        "        queries = K.repeat_elements(query, keys_len, 1)\n",
        "\n",
        "        att_input = tf.concat(\n",
        "            [queries, keys, queries - keys, queries * keys], axis=-1)\n",
        "\n",
        "        att_out = self.dnn(att_input, training=training)\n",
        "\n",
        "        attention_score = tf.nn.bias_add(tf.tensordot(att_out, self.kernel, axes=(-1, 0)), self.bias)\n",
        "\n",
        "        return attention_score\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[1][:2] + (1,)\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return mask\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
        "                  'l2_reg': self.l2_reg, 'dropout_rate': self.dropout_rate, 'use_bn': self.use_bn, 'seed': self.seed}\n",
        "        base_config = super(LocalActivationUnit, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wNH9YTcaBed"
      },
      "outputs": [],
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "Author:\n",
        "    Weichen Shen,weichenswc@163.com\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.ops.init_ops import TruncatedNormal, Constant, glorot_uniform_initializer as glorot_uniform\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.init_ops_v2 import TruncatedNormal, Constant, glorot_uniform\n",
        "\n",
        "from tensorflow.python.keras.layers import Lambda, Layer, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "#from .core import LocalActivationUnit|\n",
        "#from .normalization import LayerNormalization\n",
        "\n",
        "# if tf.__version__ >= '2.0.0':\n",
        "#     from ..contrib.rnn_v2 import dynamic_rnn\n",
        "# else:\n",
        "#     from ..contrib.rnn import dynamic_rnn\n",
        "#from ..contrib.utils import QAAttGRUCell, VecAttGRUCell\n",
        "#from .utils import reduce_sum, reduce_max, div, softmax, reduce_mean\n",
        "\n",
        "\n",
        "class SequencePoolingLayer(Layer):\n",
        "    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n",
        "\n",
        "      Input shape\n",
        "        - A list of two  tensor [seq_value,seq_len]\n",
        "\n",
        "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n",
        "\n",
        "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
        "\n",
        "      Output shape\n",
        "        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
        "\n",
        "      Arguments\n",
        "        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n",
        "\n",
        "        - **supports_masking**:If True,the input need to support masking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n",
        "\n",
        "        if mode not in ['sum', 'mean', 'max']:\n",
        "            raise ValueError(\"mode must be sum or mean\")\n",
        "        self.mode = mode\n",
        "        self.eps = tf.constant(1e-8, tf.float32)\n",
        "        super(SequencePoolingLayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.supports_masking = supports_masking\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not self.supports_masking:\n",
        "            self.seq_len_max = int(input_shape[0][1])\n",
        "        super(SequencePoolingLayer, self).build(\n",
        "            input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, seq_value_len_list, mask=None, **kwargs):\n",
        "        if self.supports_masking:\n",
        "            if mask is None:\n",
        "                raise ValueError(\n",
        "                    \"When supports_masking=True,input must support masking\")\n",
        "            uiseq_embed_list = seq_value_len_list\n",
        "            mask = tf.cast(mask, tf.float32)  # tf.to_float(mask)\n",
        "            user_behavior_length = reduce_sum(mask, axis=-1, keep_dims=True)\n",
        "            mask = tf.expand_dims(mask, axis=2)\n",
        "        else:\n",
        "            uiseq_embed_list, user_behavior_length = seq_value_len_list\n",
        "\n",
        "            mask = tf.sequence_mask(user_behavior_length,\n",
        "                                    self.seq_len_max, dtype=tf.float32)\n",
        "            mask = tf.transpose(mask, (0, 2, 1))\n",
        "\n",
        "        embedding_size = uiseq_embed_list.shape[-1]\n",
        "\n",
        "        mask = tf.tile(mask, [1, 1, embedding_size])\n",
        "\n",
        "        if self.mode == \"max\":\n",
        "            hist = uiseq_embed_list - (1 - mask) * 1e9\n",
        "            return reduce_max(hist, 1, keep_dims=True)\n",
        "\n",
        "        hist = reduce_sum(uiseq_embed_list * mask, 1, keep_dims=False)\n",
        "\n",
        "        if self.mode == \"mean\":\n",
        "            hist = div(hist, tf.cast(user_behavior_length, tf.float32) + self.eps)\n",
        "\n",
        "        hist = tf.expand_dims(hist, axis=1)\n",
        "        return hist\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.supports_masking:\n",
        "            return (None, 1, input_shape[-1])\n",
        "        else:\n",
        "            return (None, 1, input_shape[0][-1])\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'mode': self.mode, 'supports_masking': self.supports_masking}\n",
        "        base_config = super(SequencePoolingLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class WeightedSequenceLayer(Layer):\n",
        "    \"\"\"The WeightedSequenceLayer is used to apply weight score on variable-length sequence feature/multi-value feature.\n",
        "\n",
        "      Input shape\n",
        "        - A list of two  tensor [seq_value,seq_len,seq_weight]\n",
        "\n",
        "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n",
        "\n",
        "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
        "\n",
        "        - seq_weight is a 3D tensor with shape: ``(batch_size, T, 1)``\n",
        "\n",
        "      Output shape\n",
        "        - 3D tensor with shape: ``(batch_size, T, embedding_size)``.\n",
        "\n",
        "      Arguments\n",
        "        - **weight_normalization**: bool.Whether normalize the weight score before applying to sequence.\n",
        "\n",
        "        - **supports_masking**:If True,the input need to support masking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight_normalization=True, supports_masking=False, **kwargs):\n",
        "        super(WeightedSequenceLayer, self).__init__(**kwargs)\n",
        "        self.weight_normalization = weight_normalization\n",
        "        self.supports_masking = supports_masking\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not self.supports_masking:\n",
        "            self.seq_len_max = int(input_shape[0][1])\n",
        "        super(WeightedSequenceLayer, self).build(\n",
        "            input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, input_list, mask=None, **kwargs):\n",
        "        if self.supports_masking:\n",
        "            if mask is None:\n",
        "                raise ValueError(\n",
        "                    \"When supports_masking=True,input must support masking\")\n",
        "            key_input, value_input = input_list\n",
        "            mask = tf.expand_dims(mask[0], axis=2)\n",
        "        else:\n",
        "            key_input, key_length_input, value_input = input_list\n",
        "            mask = tf.sequence_mask(key_length_input,\n",
        "                                    self.seq_len_max, dtype=tf.bool)\n",
        "            mask = tf.transpose(mask, (0, 2, 1))\n",
        "\n",
        "        embedding_size = key_input.shape[-1]\n",
        "\n",
        "        if self.weight_normalization:\n",
        "            paddings = tf.ones_like(value_input) * (-2 ** 32 + 1)\n",
        "        else:\n",
        "            paddings = tf.zeros_like(value_input)\n",
        "        value_input = tf.where(mask, value_input, paddings)\n",
        "\n",
        "        if self.weight_normalization:\n",
        "            value_input = softmax(value_input, dim=1)\n",
        "\n",
        "        if len(value_input.shape) == 2:\n",
        "            value_input = tf.expand_dims(value_input, axis=2)\n",
        "            value_input = tf.tile(value_input, [1, 1, embedding_size])\n",
        "\n",
        "        return tf.multiply(key_input, value_input)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        if self.supports_masking:\n",
        "            return mask[0]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'weight_normalization': self.weight_normalization, 'supports_masking': self.supports_masking}\n",
        "        base_config = super(WeightedSequenceLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMaENs2Xcint"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.lookup_ops import TextFileInitializer\n",
        "try:\n",
        "    from tensorflow.python.ops.lookup_ops import StaticHashTable\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.lookup_ops import HashTable as StaticHashTable\n",
        "\n",
        "class Hash(Layer):\n",
        "    \"\"\"Looks up keys in a table when setup `vocabulary_path`, which outputs the corresponding values.\n",
        "    If `vocabulary_path` is not set, `Hash` will hash the input to [0,num_buckets). When `mask_zero` = True,\n",
        "    input value `0` or `0.0` will be set to `0`, and other value will be set in range [1,num_buckets).\n",
        "\n",
        "    The following snippet initializes a `Hash` with `vocabulary_path` file with the first column as keys and\n",
        "    second column as values:\n",
        "\n",
        "    * `1,emerson`\n",
        "    * `2,lake`\n",
        "    * `3,palmer`\n",
        "\n",
        "    >>> hash = Hash(\n",
        "    ...   num_buckets=3+1,\n",
        "    ...   vocabulary_path=filename,\n",
        "    ...   default_value=0)\n",
        "    >>> hash(tf.constant('lake')).numpy()\n",
        "    2\n",
        "    >>> hash(tf.constant('lakeemerson')).numpy()\n",
        "    0\n",
        "\n",
        "    Args:\n",
        "        num_buckets: An `int` that is >= 1. The number of buckets or the vocabulary size + 1\n",
        "            when `vocabulary_path` is setup.\n",
        "        mask_zero: default is False. The `Hash` value will hash input `0` or `0.0` to value `0` when\n",
        "            the `mask_zero` is `True`. `mask_zero` is not used when `vocabulary_path` is setup.\n",
        "        vocabulary_path: default `None`. The `CSV` text file path of the vocabulary hash, which contains\n",
        "            two columns seperated by delimiter `comma`, the first column is the value and the second is\n",
        "            the key. The key data type is `string`, the value data type is `int`. The path must\n",
        "            be accessible from wherever `Hash` is initialized.\n",
        "        default_value: default '0'. The default value if a key is missing in the table.\n",
        "        **kwargs: Additional keyword arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_buckets, mask_zero=False, vocabulary_path=None, default_value=0, **kwargs):\n",
        "        self.num_buckets = num_buckets\n",
        "        self.mask_zero = mask_zero\n",
        "        self.vocabulary_path = vocabulary_path\n",
        "        self.default_value = default_value\n",
        "        if self.vocabulary_path:\n",
        "            initializer = TextFileInitializer(vocabulary_path, 'string', 1, 'int64', 0, delimiter=',')\n",
        "            self.hash_table = StaticHashTable(initializer, default_value=self.default_value)\n",
        "        super(Hash, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Be sure to call this somewhere!\n",
        "        super(Hash, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None, **kwargs):\n",
        "\n",
        "        if x.dtype != tf.string:\n",
        "            zero = tf.as_string(tf.zeros([1], dtype=x.dtype))\n",
        "            x = tf.as_string(x, )\n",
        "        else:\n",
        "            zero = tf.as_string(tf.zeros([1], dtype='int32'))\n",
        "\n",
        "        if self.vocabulary_path:\n",
        "            hash_x = self.hash_table.lookup(x)\n",
        "            return hash_x\n",
        "\n",
        "        num_buckets = self.num_buckets if not self.mask_zero else self.num_buckets - 1\n",
        "        try:\n",
        "            hash_x = tf.string_to_hash_bucket_fast(x, num_buckets,\n",
        "                                                   name=None)  # weak hash\n",
        "        except AttributeError:\n",
        "            hash_x = tf.strings.to_hash_bucket_fast(x, num_buckets,\n",
        "                                                    name=None)  # weak hash\n",
        "        if self.mask_zero:\n",
        "            mask = tf.cast(tf.not_equal(x, zero), dtype='int64')\n",
        "            hash_x = (hash_x + 1) * mask\n",
        "\n",
        "        return hash_x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'num_buckets': self.num_buckets, 'mask_zero': self.mask_zero, 'vocabulary_path': self.vocabulary_path,\n",
        "                  'default_value': self.default_value}\n",
        "        base_config = super(Hash, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ylxxDUZerg"
      },
      "outputs": [],
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "Author:\n",
        "    Weichen Shen,weichenswc@163.com\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "\n",
        "from tensorflow.python.keras.layers import Embedding, Lambda\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "\n",
        "#from .layers.sequence import SequencePoolingLayer, WeightedSequenceLayer\n",
        "#from .layers.utils import Hash\n",
        "\n",
        "\n",
        "def get_inputs_list(inputs):\n",
        "    return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))\n",
        "\n",
        "\n",
        "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed, l2_reg,\n",
        "                          prefix='sparse_', seq_mask_zero=True):\n",
        "    sparse_embedding = {}\n",
        "    for feat in sparse_feature_columns:\n",
        "        emb = Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
        "                        embeddings_initializer=feat.embeddings_initializer,\n",
        "                        embeddings_regularizer=l2(l2_reg),\n",
        "                        name=prefix + '_emb_' + feat.embedding_name)\n",
        "        emb.trainable = feat.trainable\n",
        "        sparse_embedding[feat.embedding_name] = emb\n",
        "\n",
        "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
        "        for feat in varlen_sparse_feature_columns:\n",
        "            # if feat.name not in sparse_embedding:\n",
        "            emb = Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
        "                            embeddings_initializer=feat.embeddings_initializer,\n",
        "                            embeddings_regularizer=l2(\n",
        "                                l2_reg),\n",
        "                            name=prefix + '_seq_emb_' + feat.name,\n",
        "                            mask_zero=seq_mask_zero)\n",
        "            emb.trainable = feat.trainable\n",
        "            sparse_embedding[feat.embedding_name] = emb\n",
        "    return sparse_embedding\n",
        "\n",
        "\n",
        "def get_embedding_vec_list(embedding_dict, input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):\n",
        "    embedding_vec_list = []\n",
        "    for fg in sparse_feature_columns:\n",
        "        feat_name = fg.name\n",
        "        if len(return_feat_list) == 0 or feat_name in return_feat_list:\n",
        "            if fg.use_hash:\n",
        "                lookup_idx = Hash(fg.vocabulary_size, mask_zero=(feat_name in mask_feat_list), vocabulary_path=fg.vocabulary_path)(input_dict[feat_name])\n",
        "            else:\n",
        "                lookup_idx = input_dict[feat_name]\n",
        "\n",
        "            embedding_vec_list.append(embedding_dict[feat_name](lookup_idx))\n",
        "\n",
        "    return embedding_vec_list\n",
        "\n",
        "\n",
        "def create_embedding_matrix(feature_columns, l2_reg, seed, prefix=\"\", seq_mask_zero=True):\n",
        "    #from . import feature_column as fc_lib\n",
        "\n",
        "    sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
        "    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed,\n",
        "                                            l2_reg, prefix=prefix + 'sparse', seq_mask_zero=seq_mask_zero)\n",
        "    return sparse_emb_dict\n",
        "\n",
        "\n",
        "def embedding_lookup(sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),\n",
        "                     mask_feat_list=(), to_list=False):\n",
        "    group_embedding_dict = defaultdict(list)\n",
        "    for fc in sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        embedding_name = fc.embedding_name\n",
        "        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n",
        "            if fc.use_hash:\n",
        "                lookup_idx = Hash(fc.vocabulary_size, mask_zero=(feature_name in mask_feat_list), vocabulary_path=fc.vocabulary_path)(\n",
        "                    sparse_input_dict[feature_name])\n",
        "            else:\n",
        "                lookup_idx = sparse_input_dict[feature_name]\n",
        "\n",
        "            group_embedding_dict[fc.group_name].append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
        "    if to_list:\n",
        "        return list(chain.from_iterable(group_embedding_dict.values()))\n",
        "    return group_embedding_dict\n",
        "\n",
        "\n",
        "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
        "    varlen_embedding_vec_dict = {}\n",
        "    for fc in varlen_sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        embedding_name = fc.embedding_name\n",
        "        if fc.use_hash:\n",
        "            lookup_idx = Hash(fc.vocabulary_size, mask_zero=True, vocabulary_path=fc.vocabulary_path)(sequence_input_dict[feature_name])\n",
        "        else:\n",
        "            lookup_idx = sequence_input_dict[feature_name]\n",
        "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
        "    return varlen_embedding_vec_dict\n",
        "\n",
        "\n",
        "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns, to_list=False):\n",
        "    pooling_vec_list = defaultdict(list)\n",
        "    for fc in varlen_sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        combiner = fc.combiner\n",
        "        feature_length_name = fc.length_name\n",
        "        if feature_length_name is not None:\n",
        "            if fc.weight_name is not None:\n",
        "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm)(\n",
        "                    [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])\n",
        "            else:\n",
        "                seq_input = embedding_dict[feature_name]\n",
        "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
        "                [seq_input, features[feature_length_name]])\n",
        "        else:\n",
        "            if fc.weight_name is not None:\n",
        "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm, supports_masking=True)(\n",
        "                    [embedding_dict[feature_name], features[fc.weight_name]])\n",
        "            else:\n",
        "                seq_input = embedding_dict[feature_name]\n",
        "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
        "                seq_input)\n",
        "        pooling_vec_list[fc.group_name].append(vec)\n",
        "    if to_list:\n",
        "        return chain.from_iterable(pooling_vec_list.values())\n",
        "    return pooling_vec_list\n",
        "\n",
        "\n",
        "def get_dense_input(features, feature_columns):\n",
        "    #from . import feature_column as fc_lib\n",
        "    dense_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if feature_columns else []\n",
        "    dense_input_list = []\n",
        "    for fc in dense_feature_columns:\n",
        "        if fc.transform_fn is None:\n",
        "            dense_input_list.append(features[fc.name])\n",
        "        else:\n",
        "            transform_result = Lambda(fc.transform_fn)(features[fc.name])\n",
        "            dense_input_list.append(transform_result)\n",
        "    return dense_input_list\n",
        "\n",
        "\n",
        "def mergeDict(a, b):\n",
        "    c = defaultdict(list)\n",
        "    for k, v in a.items():\n",
        "        c[k].extend(v)\n",
        "    for k, v in b.items():\n",
        "        c[k].extend(v)\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHQlQL9teUX-"
      },
      "outputs": [],
      "source": [
        "class Concat(Layer):\n",
        "    def __init__(self, axis, supports_masking=True, **kwargs):\n",
        "        super(Concat, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "        self.supports_masking = supports_masking\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # target_shape = tf.shape(inputs[0])\n",
        "        # inputs = [tf.reshape(input, target_shape) for input in inputs]\n",
        "        # inputs = [tf.cast(input, tf.float32) for input in inputs]\n",
        "        # return tf.concat(inputs, axis=self.axis)\n",
        "        # inputs = [tf.cast(input, tf.float32) for input in inputs]# zjh_edit\n",
        "        return tf.concat(inputs, axis=self.axis)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if not self.supports_masking:\n",
        "            return None\n",
        "        if mask is None:\n",
        "            mask = [inputs_i._keras_mask if hasattr(inputs_i, \"_keras_mask\") else None for inputs_i in inputs]\n",
        "        if mask is None:\n",
        "            return None\n",
        "        if not isinstance(mask, list):\n",
        "            raise ValueError('`mask` should be a list.')\n",
        "        if not isinstance(inputs, list):\n",
        "            raise ValueError('`inputs` should be a list.')\n",
        "        if len(mask) != len(inputs):\n",
        "            raise ValueError('The lists `inputs` and `mask` '\n",
        "                             'should have the same length.')\n",
        "        if all([m is None for m in mask]):\n",
        "            return None\n",
        "        # Make a list of masks while making sure\n",
        "        # the dimensionality of each mask\n",
        "        # is the same as the corresponding input.\n",
        "        masks = []\n",
        "        for input_i, mask_i in zip(inputs, mask):\n",
        "            if mask_i is None:\n",
        "                # Input is unmasked. Append all 1s to masks,\n",
        "                masks.append(tf.ones_like(input_i, dtype='bool'))\n",
        "            elif K.ndim(mask_i) < K.ndim(input_i):\n",
        "                # Mask is smaller than the input, expand it\n",
        "                masks.append(tf.expand_dims(mask_i, axis=-1))\n",
        "            else:\n",
        "                masks.append(mask_i)\n",
        "        concatenated = K.concatenate(masks, axis=self.axis)\n",
        "        return K.all(concatenated, axis=-1, keepdims=False)\n",
        "class NoMask(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NoMask, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Be sure to call this somewhere!\n",
        "        super(NoMask, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None, **kwargs):\n",
        "        return x\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "\n",
        "def concat_func(inputs, axis=-1, mask=False):\n",
        "    if len(inputs) == 1:\n",
        "        input = inputs[0]\n",
        "        if not mask:\n",
        "            input = NoMask()(input)\n",
        "        return input\n",
        "    return Concat(axis, supports_masking=mask)(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06l40bi7e7Lc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from tensorflow.python.ops.init_ops import Zeros, glorot_normal_initializer as glorot_normal\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.init_ops_v2 import Zeros, glorot_normal\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, l2_reg=0.0, mode=0, use_bias=False, seed=1024, **kwargs):\n",
        "\n",
        "        self.l2_reg = l2_reg\n",
        "        # self.l2_reg = tf.contrib.layers.l2_regularizer(float(l2_reg_linear))\n",
        "        if mode not in [0, 1, 2]:\n",
        "            raise ValueError(\"mode must be 0,1 or 2\")\n",
        "        self.mode = mode\n",
        "        self.use_bias = use_bias\n",
        "        self.seed = seed\n",
        "        super(Linear, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name='linear_bias',\n",
        "                                        shape=(1,),\n",
        "                                        initializer=Zeros(),\n",
        "                                        trainable=True)\n",
        "        if self.mode == 1:\n",
        "            self.kernel = self.add_weight(\n",
        "                'linear_kernel',\n",
        "                shape=[int(input_shape[-1]), 1],\n",
        "                initializer=glorot_normal(self.seed),\n",
        "                regularizer=l2(self.l2_reg),\n",
        "                trainable=True)\n",
        "        elif self.mode == 2:\n",
        "            self.kernel = self.add_weight(\n",
        "                'linear_kernel',\n",
        "                shape=[int(input_shape[1][-1]), 1],\n",
        "                initializer=glorot_normal(self.seed),\n",
        "                regularizer=l2(self.l2_reg),\n",
        "                trainable=True)\n",
        "\n",
        "        super(Linear, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.mode == 0:\n",
        "            sparse_input = inputs\n",
        "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=True)\n",
        "        elif self.mode == 1:\n",
        "            dense_input = inputs\n",
        "            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))\n",
        "            linear_logit = fc\n",
        "        else:\n",
        "            sparse_input, dense_input = inputs\n",
        "            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))\n",
        "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=False) + fc\n",
        "        if self.use_bias:\n",
        "            linear_logit += self.bias\n",
        "\n",
        "        return linear_logit\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1)\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'mode': self.mode, 'l2_reg': self.l2_reg, 'use_bias': self.use_bias, 'seed': self.seed}\n",
        "        base_config = super(Linear, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-kb7RVIY3Hv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from collections import namedtuple, OrderedDict\n",
        "from copy import copy\n",
        "from itertools import chain\n",
        "\n",
        "from tensorflow.python.keras.initializers import RandomNormal, Zeros\n",
        "from tensorflow.python.keras.layers import Input, Lambda\n",
        "\n",
        "# from .inputs import create_embedding_matrix, embedding_lookup, get_dense_input, varlen_embedding_lookup, \\\n",
        "#     get_varlen_pooling_list, mergeDict\n",
        "#from .layers import Linear\n",
        "#from .layers.utils import concat_func\n",
        "\n",
        "DEFAULT_GROUP_NAME = \"default_group\"\n",
        "\n",
        "\n",
        "class SparseFeat(namedtuple('SparseFeat',\n",
        "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'vocabulary_path', 'dtype', 'embeddings_initializer',\n",
        "                             'embedding_name',\n",
        "                             'group_name', 'trainable'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype=\"int32\", embeddings_initializer=None,\n",
        "                embedding_name=None,\n",
        "                group_name=DEFAULT_GROUP_NAME, trainable=True):\n",
        "\n",
        "        if embedding_dim == \"auto\":\n",
        "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
        "        if embeddings_initializer is None:\n",
        "            embeddings_initializer = RandomNormal(mean=0.0, stddev=0.0001, seed=2020)\n",
        "\n",
        "        if embedding_name is None:\n",
        "            embedding_name = name\n",
        "\n",
        "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, vocabulary_path, dtype,\n",
        "                                              embeddings_initializer,\n",
        "                                              embedding_name, group_name, trainable)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return self.name.__hash__()\n",
        "\n",
        "\n",
        "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
        "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name', 'weight_name', 'weight_norm'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None, weight_name=None, weight_norm=True):\n",
        "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name, weight_name,\n",
        "                                                    weight_norm)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.sparsefeat.name\n",
        "\n",
        "    @property\n",
        "    def vocabulary_size(self):\n",
        "        return self.sparsefeat.vocabulary_size\n",
        "\n",
        "    @property\n",
        "    def embedding_dim(self):\n",
        "        return self.sparsefeat.embedding_dim\n",
        "\n",
        "    @property\n",
        "    def use_hash(self):\n",
        "        return self.sparsefeat.use_hash\n",
        "\n",
        "    @property\n",
        "    def vocabulary_path(self):\n",
        "        return self.sparsefeat.vocabulary_path\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.sparsefeat.dtype\n",
        "\n",
        "    @property\n",
        "    def embeddings_initializer(self):\n",
        "        return self.sparsefeat.embeddings_initializer\n",
        "\n",
        "    @property\n",
        "    def embedding_name(self):\n",
        "        return self.sparsefeat.embedding_name\n",
        "\n",
        "    @property\n",
        "    def group_name(self):\n",
        "        return self.sparsefeat.group_name\n",
        "\n",
        "    @property\n",
        "    def trainable(self):\n",
        "        return self.sparsefeat.trainable\n",
        "\n",
        "    def __hash__(self):\n",
        "        return self.name.__hash__()\n",
        "\n",
        "\n",
        "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype', 'transform_fn'])):\n",
        "    \"\"\" Dense feature\n",
        "    Args:\n",
        "        name: feature name.\n",
        "        dimension: dimension of the feature, default = 1.\n",
        "        dtype: dtype of the feature, default=\"float32\".\n",
        "        transform_fn: If not `None` , a function that can be used to transform\n",
        "        values of the feature.  the function takes the input Tensor as its\n",
        "        argument, and returns the output Tensor.\n",
        "        (e.g. lambda x: (x - 3.0) / 4.2).\n",
        "    \"\"\"\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, name, dimension=1, dtype=\"float32\", transform_fn=None):\n",
        "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype, transform_fn)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return self.name.__hash__()\n",
        "\n",
        "    # def __eq__(self, other):\n",
        "    #     if self.name == other.name:\n",
        "    #         return True\n",
        "    #     return False\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     return 'DenseFeat:'+self.name\n",
        "\n",
        "\n",
        "def get_feature_names(feature_columns):\n",
        "    features = build_input_features(feature_columns)\n",
        "    return list(features.keys())\n",
        "\n",
        "\n",
        "def build_input_features(feature_columns, prefix=''):\n",
        "    \"\"\"\n",
        "    构建模型输入特征。\n",
        "\n",
        "    该函数根据传入的特征列列表，生成对应的Keras模型输入层。特征列可以是稀疏特征、密集特征或变长稀疏特征。\n",
        "    对于每种类型的特征，函数会创建一个相应的Input层，并根据特征的类型和属性设置适当的形状、名称和数据类型。\n",
        "\n",
        "    参数：\n",
        "    feature_columns (list): 特征列列表，包含SparseFeat、DenseFeat和VarLenSparseFeat对象。\n",
        "    prefix (str): 输入层名称的前缀，用于区分不同模型的输入层。默认为空字符串。\n",
        "\n",
        "    返回：\n",
        "    OrderedDict: 包含构建好的Keras Input层的有序字典，键为特征名称，值为对应的Input层。\n",
        "\n",
        "    异常：\n",
        "    TypeError: 如果feature_columns中包含无效的特征列类型。\n",
        "\n",
        "    样例：\n",
        "    >>> from collections import OrderedDict\n",
        "    >>> class SparseFeat:\n",
        "    ...     def __init__(self, name, dtype):\n",
        "    ...         self.name = name\n",
        "    ...         self.dtype = dtype\n",
        "    >>> class DenseFeat:\n",
        "    ...     def __init__(self, name, dimension, dtype):\n",
        "    ...         self.name = name\n",
        "    ...         self.dimension = dimension\n",
        "    ...         self.dtype = dtype\n",
        "    >>> class VarLenSparseFeat:\n",
        "    ...     def __init__(self, name, maxlen, dtype, weight_name=None, length_name=None):\n",
        "    ...         self.name = name\n",
        "    ...         self.maxlen = maxlen\n",
        "    ...         self.dtype = dtype\n",
        "    ...         self.weight_name = weight_name\n",
        "    ...         self.length_name = length_name\n",
        "    >>> feature_columns = [\n",
        "    ...     SparseFeat('user_id', 'int32'),\n",
        "    ...     DenseFeat('age', 1, 'float32'),\n",
        "    ...     VarLenSparseFeat('search_history', 10, 'int32', 'search_history_weight', 'search_history_length')\n",
        "    ... ]\n",
        "    >>> input_features = build_input_features(feature_columns, prefix='input_')\n",
        "    >>> print(input_features)\n",
        "    OrderedDict([\n",
        "        ('input_user_id', <keras.layers.InputLayer ...>),\n",
        "        ('input_age', <keras.layers.InputLayer ...>),\n",
        "        ('input_search_history', <keras.layers.InputLayer ...>),\n",
        "        ('input_search_history_weight', <keras.layers.InputLayer ...>),\n",
        "        ('input_search_history_length', <keras.layers.InputLayer ...>)\n",
        "    ])\n",
        "    \"\"\"\n",
        "    input_features = OrderedDict()\n",
        "    for fc in feature_columns:\n",
        "        if isinstance(fc, SparseFeat):\n",
        "            input_features[fc.name] = Input(\n",
        "                shape=(1,), name=prefix + fc.name, dtype=fc.dtype)\n",
        "        elif isinstance(fc, DenseFeat):\n",
        "            input_features[fc.name] = Input(\n",
        "                shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
        "        elif isinstance(fc, VarLenSparseFeat):\n",
        "            input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + fc.name,\n",
        "                                            dtype=fc.dtype)\n",
        "            if fc.weight_name is not None:\n",
        "                input_features[fc.weight_name] = Input(shape=(fc.maxlen, 1), name=prefix + fc.weight_name,\n",
        "                                                       dtype=\"float32\")\n",
        "            if fc.length_name is not None:\n",
        "                input_features[fc.length_name] = Input((1,), name=prefix + fc.length_name, dtype='int32')\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Invalid feature column type,got\", type(fc))\n",
        "\n",
        "    return input_features\n",
        "\n",
        "\n",
        "def get_linear_logit(features, feature_columns, units=1, use_bias=False, seed=1024, prefix='linear',\n",
        "                     l2_reg=0, sparse_feat_refine_weight=None):\n",
        "    linear_feature_columns = copy(feature_columns)\n",
        "    for i in range(len(linear_feature_columns)):\n",
        "        if isinstance(linear_feature_columns[i], SparseFeat):\n",
        "            linear_feature_columns[i] = linear_feature_columns[i]._replace(embedding_dim=1,\n",
        "                                                                           embeddings_initializer=Zeros())\n",
        "        if isinstance(linear_feature_columns[i], VarLenSparseFeat):\n",
        "            linear_feature_columns[i] = linear_feature_columns[i]._replace(\n",
        "                sparsefeat=linear_feature_columns[i].sparsefeat._replace(embedding_dim=1,\n",
        "                                                                         embeddings_initializer=Zeros()))\n",
        "\n",
        "    linear_emb_list = [input_from_feature_columns(features, linear_feature_columns, l2_reg, seed,\n",
        "                                                  prefix=prefix + str(i))[0] for i in range(units)]\n",
        "    _, dense_input_list = input_from_feature_columns(features, linear_feature_columns, l2_reg, seed, prefix=prefix)\n",
        "\n",
        "    linear_logit_list = []\n",
        "    for i in range(units):\n",
        "\n",
        "        if len(linear_emb_list[i]) > 0 and len(dense_input_list) > 0:\n",
        "            sparse_input = concat_func(linear_emb_list[i])\n",
        "            dense_input = concat_func(dense_input_list)\n",
        "            if sparse_feat_refine_weight is not None:\n",
        "                sparse_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(\n",
        "                    [sparse_input, sparse_feat_refine_weight])\n",
        "            linear_logit = Linear(l2_reg, mode=2, use_bias=use_bias, seed=seed)([sparse_input, dense_input])\n",
        "        elif len(linear_emb_list[i]) > 0:\n",
        "            sparse_input = concat_func(linear_emb_list[i])\n",
        "            if sparse_feat_refine_weight is not None:\n",
        "                sparse_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(\n",
        "                    [sparse_input, sparse_feat_refine_weight])\n",
        "            linear_logit = Linear(l2_reg, mode=0, use_bias=use_bias, seed=seed)(sparse_input)\n",
        "        elif len(dense_input_list) > 0:\n",
        "            dense_input = concat_func(dense_input_list)\n",
        "            linear_logit = Linear(l2_reg, mode=1, use_bias=use_bias, seed=seed)(dense_input)\n",
        "        else:   #empty feature_columns\n",
        "            return Lambda(lambda x: tf.constant([[0.0]]))(list(features.values())[0])\n",
        "        linear_logit_list.append(linear_logit)\n",
        "\n",
        "    return concat_func(linear_logit_list)\n",
        "\n",
        "\n",
        "def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,\n",
        "                               support_dense=True, support_group=False):\n",
        "    sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
        "\n",
        "    embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix,\n",
        "                                                    seq_mask_zero=seq_mask_zero)\n",
        "    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n",
        "    dense_value_list = get_dense_input(features, feature_columns)\n",
        "    if not support_dense and len(dense_value_list) > 0:\n",
        "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
        "\n",
        "    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n",
        "    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n",
        "                                                                 varlen_sparse_feature_columns)\n",
        "    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n",
        "    if not support_group:\n",
        "        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n",
        "    return group_embedding_dict, dense_value_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuNsXjTyiepo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.ops.init_ops import Zeros\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.init_ops_v2 import Zeros\n",
        "from tensorflow.python.keras.layers import Layer, Activation\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.keras.layers import BatchNormalization\n",
        "except ImportError:\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "\n",
        "try:\n",
        "    unicode\n",
        "except NameError:\n",
        "    unicode = str\n",
        "\n",
        "class Dice(Layer):\n",
        "    \"\"\"The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.\n",
        "\n",
        "      Input shape\n",
        "        - Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "      Output shape\n",
        "        - Same shape as the input.\n",
        "\n",
        "      Arguments\n",
        "        - **axis** : Integer, the axis that should be used to compute data distribution (typically the features axis).\n",
        "\n",
        "        - **epsilon** : Small float added to variance to avoid dividing by zero.\n",
        "\n",
        "      References\n",
        "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis=-1, epsilon=1e-9, **kwargs):\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        super(Dice, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bn = BatchNormalization(\n",
        "            axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
        "        self.alphas = self.add_weight(shape=(input_shape[-1],), initializer=Zeros(\n",
        "        ), dtype=tf.float32, name='dice_alpha')  # name='alpha_'+self.name\n",
        "        super(Dice, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "        self.uses_learning_phase = True\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        inputs_normed = self.bn(inputs, training=training)\n",
        "        # tf.layers.batch_normalization(\n",
        "        # inputs, axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
        "        x_p = tf.sigmoid(inputs_normed)\n",
        "        return self.alphas * (1.0 - x_p) * inputs + x_p * inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'axis': self.axis, 'epsilon': self.epsilon}\n",
        "        base_config = super(Dice, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def activation_layer(activation):\n",
        "    if activation in (\"dice\", \"Dice\"):\n",
        "        act_layer = Dice()\n",
        "    elif isinstance(activation, (str, unicode)):\n",
        "        act_layer = Activation(activation)\n",
        "    elif issubclass(activation, Layer):\n",
        "        act_layer = activation()\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid activation,found %s.You should use a str or a Activation Layer Class.\" % (activation))\n",
        "    return act_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejnOWbYcfhnf"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from tensorflow.python.keras.layers import BatchNormalization\n",
        "except ImportError:\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "\n",
        "# from .activation import activation_layer\n",
        "\n",
        "class DNN(Layer):\n",
        "    \"\"\"The Multi Layer Percetron\n",
        "\n",
        "      Input shape\n",
        "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
        "\n",
        "      Output shape\n",
        "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
        "\n",
        "      Arguments\n",
        "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
        "\n",
        "        - **activation**: Activation function to use.\n",
        "\n",
        "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
        "\n",
        "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
        "\n",
        "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
        "\n",
        "        - **output_activation**: Activation function to use in the last layer.If ``None``,it will be same as ``activation``.\n",
        "\n",
        "        - **seed**: A Python integer to use as random seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, output_activation=None,\n",
        "                 seed=1024, **kwargs):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.l2_reg = l2_reg\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_bn = use_bn\n",
        "        self.output_activation = output_activation\n",
        "        self.seed = seed\n",
        "\n",
        "        super(DNN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # if len(self.hidden_units) == 0:\n",
        "        #     raise ValueError(\"hidden_units is empty\")\n",
        "        input_size = input_shape[-1]\n",
        "        hidden_units = [int(input_size)] + list(self.hidden_units)\n",
        "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
        "                                        shape=(\n",
        "                                            hidden_units[i], hidden_units[i + 1]),\n",
        "                                        initializer=glorot_normal(\n",
        "                                            seed=self.seed),\n",
        "                                        regularizer=l2(self.l2_reg),\n",
        "                                        trainable=True) for i in range(len(self.hidden_units))]\n",
        "        self.bias = [self.add_weight(name='bias' + str(i),\n",
        "                                     shape=(self.hidden_units[i],),\n",
        "                                     initializer=Zeros(),\n",
        "                                     trainable=True) for i in range(len(self.hidden_units))]\n",
        "        if self.use_bn:\n",
        "            self.bn_layers = [BatchNormalization() for _ in range(len(self.hidden_units))]\n",
        "\n",
        "        self.dropout_layers = [Dropout(self.dropout_rate, seed=self.seed + i) for i in\n",
        "                               range(len(self.hidden_units))]\n",
        "\n",
        "        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]\n",
        "\n",
        "        if self.output_activation:\n",
        "            self.activation_layers[-1] = activation_layer(self.output_activation)\n",
        "\n",
        "        super(DNN, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "\n",
        "        deep_input = inputs\n",
        "\n",
        "        for i in range(len(self.hidden_units)):\n",
        "            fc = tf.nn.bias_add(tf.tensordot(\n",
        "                deep_input, self.kernels[i], axes=(-1, 0)), self.bias[i])\n",
        "\n",
        "            if self.use_bn:\n",
        "                fc = self.bn_layers[i](fc, training=training)\n",
        "            try:\n",
        "                fc = self.activation_layers[i](fc, training=training)\n",
        "            except TypeError as e:  # TypeError: call() got an unexpected keyword argument 'training'\n",
        "                print(\"make sure the activation function use training flag properly\", e)\n",
        "                fc = self.activation_layers[i](fc)\n",
        "\n",
        "            fc = self.dropout_layers[i](fc, training=training)\n",
        "            deep_input = fc\n",
        "\n",
        "        return deep_input\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if len(self.hidden_units) > 0:\n",
        "            shape = input_shape[:-1] + (self.hidden_units[-1],)\n",
        "        else:\n",
        "            shape = input_shape\n",
        "\n",
        "        return tuple(shape)\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
        "                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate,\n",
        "                  'output_activation': self.output_activation, 'seed': self.seed}\n",
        "        base_config = super(DNN, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class PredictionLayer(Layer):\n",
        "    \"\"\"\n",
        "      Arguments\n",
        "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
        "\n",
        "         - **use_bias**: bool.Whether add bias term or not.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
        "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
        "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
        "        self.task = task\n",
        "        self.use_bias = use_bias\n",
        "        super(PredictionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.global_bias = self.add_weight(\n",
        "                shape=(1,), initializer=Zeros(), name=\"global_bias\")\n",
        "\n",
        "        # Be sure to call this somewhere!\n",
        "        super(PredictionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        if self.use_bias:\n",
        "            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')\n",
        "        if self.task == \"binary\":\n",
        "            x = tf.sigmoid(x)\n",
        "\n",
        "        output = tf.reshape(x, (-1, 1))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1)\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'task': self.task, 'use_bias': self.use_bias}\n",
        "        base_config = super(PredictionLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he_3gNOJfySP"
      },
      "outputs": [],
      "source": [
        "class AttentionSequencePoolingLayer(Layer):\n",
        "  \"\"\"\n",
        "    实现 Deep Interest Network (DIN) 中使用的注意力序列池化操作。\n",
        "\n",
        "    该层用于处理查询（query）和键（keys）的序列，通过注意力机制对键序列进行加权求和，以获得与查询相关的表示。\n",
        "\n",
        "    参数：\n",
        "    - att_hidden_units (tuple): 整数列表，表示注意力网络中每层的单元数。\n",
        "    - att_activation (str): 注意力网络中使用的激活函数，默认为 'sigmoid'。\n",
        "    - weight_normalization (bool): 是否对局部激活单元的注意力分数进行归一化，默认为 False。\n",
        "    - return_score (bool): 是否返回注意力分数，默认为 False。\n",
        "    - supports_masking (bool): 输入是否支持掩码，默认为 False。\n",
        "\n",
        "    输入形状：\n",
        "    - 一个包含三个张量的列表：[query, keys, keys_length]\n",
        "    - query 是一个 3D 张量，形状为：(batch_size, 1, embedding_size)\n",
        "    - keys 是一个 3D 张量，形状为：(batch_size, T, embedding_size)\n",
        "    - keys_length 是一个 2D 张量，形状为：(batch_size, 1)\n",
        "\n",
        "    输出形状：\n",
        "    - 一个 3D 张量，形状为：(batch_size, 1, embedding_size)\n",
        "    样例：\n",
        "    >>> from tensorflow.keras.layers import Input\n",
        "    >>> query = Input(shape=(1, 40))  # 假设 embedding_size 为 40\n",
        "    >>> keys = Input(shape=(10, 40))   # 假设序列长度 T 为 10\n",
        "    >>> keys_length = Input(shape=(1,))\n",
        "    >>> attention_layer = AttentionSequencePoolingLayer(att_hidden_units=(80, 40), att_activation='sigmoid')\n",
        "    >>> output = attention_layer([query, keys, keys_length])\n",
        "    >>> print(output.shape)\n",
        "    (None, 1, 40)\n",
        "\n",
        "    注意：\n",
        "    - 当 supports_masking=True 时，输入必须支持掩码。\n",
        "    - 当 weight_normalization=True 时，会对注意力分数进行归一化处理。\n",
        "    \"\"\"\n",
        "    def __init__(self, att_hidden_units=(80, 40), att_activation='sigmoid', weight_normalization=False,\n",
        "                 return_score=False,\n",
        "                 supports_masking=False, **kwargs):\n",
        "\n",
        "        self.att_hidden_units = att_hidden_units\n",
        "        self.att_activation = att_activation\n",
        "        self.weight_normalization = weight_normalization\n",
        "        self.return_score = return_score\n",
        "        super(AttentionSequencePoolingLayer, self).__init__(**kwargs)\n",
        "        self.supports_masking = supports_masking\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not self.supports_masking:\n",
        "            if not isinstance(input_shape, list) or len(input_shape) != 3:\n",
        "                raise ValueError('A `AttentionSequencePoolingLayer` layer should be called '\n",
        "                                 'on a list of 3 inputs')\n",
        "\n",
        "            if len(input_shape[0]) != 3 or len(input_shape[1]) != 3 or len(input_shape[2]) != 2:\n",
        "                raise ValueError(\n",
        "                    \"Unexpected inputs dimensions,the 3 tensor dimensions are %d,%d and %d , expect to be 3,3 and 2\" % (\n",
        "                        len(input_shape[0]), len(input_shape[1]), len(input_shape[2])))\n",
        "\n",
        "            if input_shape[0][-1] != input_shape[1][-1] or input_shape[0][1] != 1 or input_shape[2][1] != 1:\n",
        "                raise ValueError('A `AttentionSequencePoolingLayer` layer requires '\n",
        "                                 'inputs of a 3 tensor with shape (None,1,embedding_size),(None,T,embedding_size) and (None,1)'\n",
        "                                 'Got different shapes: %s' % (input_shape))\n",
        "        else:\n",
        "            pass\n",
        "        self.local_att = LocalActivationUnit(\n",
        "            self.att_hidden_units, self.att_activation, l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, )\n",
        "        super(AttentionSequencePoolingLayer, self).build(\n",
        "            input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None, **kwargs):\n",
        "\n",
        "        if self.supports_masking:\n",
        "            if mask is None:\n",
        "                raise ValueError(\n",
        "                    \"When supports_masking=True,input must support masking\")\n",
        "            queries, keys = inputs\n",
        "            key_masks = tf.expand_dims(mask[-1], axis=1)\n",
        "\n",
        "        else:\n",
        "\n",
        "            queries, keys, keys_length = inputs\n",
        "            hist_len = keys.get_shape()[1]\n",
        "            key_masks = tf.sequence_mask(keys_length, hist_len)\n",
        "\n",
        "        attention_score = self.local_att([queries, keys], training=training)\n",
        "\n",
        "        outputs = tf.transpose(attention_score, (0, 2, 1))\n",
        "\n",
        "        if self.weight_normalization:\n",
        "            paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
        "        else:\n",
        "            paddings = tf.zeros_like(outputs)\n",
        "\n",
        "        outputs = tf.where(key_masks, outputs, paddings)\n",
        "\n",
        "        if self.weight_normalization:\n",
        "            outputs = softmax(outputs)\n",
        "\n",
        "        if not self.return_score:\n",
        "            outputs = tf.matmul(outputs, keys)\n",
        "\n",
        "        if tf.__version__ < '1.13.0':\n",
        "            outputs._uses_learning_phase = attention_score._uses_learning_phase\n",
        "        else:\n",
        "            outputs._uses_learning_phase = training is not None\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_score:\n",
        "            return (None, 1, input_shape[1][1])\n",
        "        else:\n",
        "            return (None, 1, input_shape[0][-1])\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "\n",
        "    def get_config(self, ):\n",
        "\n",
        "        config = {'att_hidden_units': self.att_hidden_units, 'att_activation': self.att_activation,\n",
        "                  'weight_normalization': self.weight_normalization, 'return_score': self.return_score,\n",
        "                  'supports_masking': self.supports_masking}\n",
        "        base_config = super(AttentionSequencePoolingLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf1VRojIgcIn"
      },
      "outputs": [],
      "source": [
        "def combined_dnn_input(sparse_embedding_list, dense_value_list):\n",
        "    \"\"\"\n",
        "    合并稀疏嵌入列表和密集值列表，为深度神经网络（DNN）提供输入。\n",
        "\n",
        "    该函数将稀疏特征的嵌入向量和密集特征值列表合并，以创建一个统一的输入层，供DNN使用。\n",
        "    如果同时存在稀疏嵌入和密集值，它们将被分别拼接和展平，然后合并在一起。\n",
        "    如果只存在其中一种类型的特征，将只使用该类型的特征作为DNN的输入。\n",
        "\n",
        "    参数：\n",
        "    sparse_embedding_list (list): 稀疏特征的嵌入向量列表。\n",
        "    dense_value_list (list): 密集特征值列表。\n",
        "\n",
        "    返回：\n",
        "    keras.layers.Layer: 合并后的DNN输入层。\n",
        "\n",
        "    异常：\n",
        "    NotImplementedError: 如果输入列表为空，即没有稀疏嵌入也没有密集值。\n",
        "\n",
        "    样例：\n",
        "    >>> from keras.layers import Input, Embedding, Dense\n",
        "    >>> # 假设sparse_embedding_list包含两个稀疏特征的嵌入向量\n",
        "    >>> sparse_emb1 = Input(shape=(1,), name='sparse_emb1')\n",
        "    >>> sparse_emb2 = Input(shape=(1,), name='sparse_emb2')\n",
        "    >>> sparse_embedding_list = [sparse_emb1, sparse_emb2]\n",
        "    >>> # 假设dense_value_list包含一个密集特征值\n",
        "    >>> dense_value = Input(shape=(1,), name='dense_value')\n",
        "    >>> dense_value_list = [dense_value]\n",
        "    >>> dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
        "    >>> print(dnn_input)  # 将输出合并后的DNN输入层\n",
        "    \"\"\"\n",
        "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
        "        sparse_dnn_input = Flatten()(concat_func(sparse_embedding_list))\n",
        "        dense_dnn_input = Flatten()(concat_func(dense_value_list))\n",
        "        return concat_func([sparse_dnn_input, dense_dnn_input])\n",
        "    elif len(sparse_embedding_list) > 0:\n",
        "        return Flatten()(concat_func(sparse_embedding_list))\n",
        "    elif len(dense_value_list) > 0:\n",
        "        return Flatten()(concat_func(dense_value_list))\n",
        "    else:\n",
        "        raise NotImplementedError(\"dnn_feature_columns can not be empty list\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJKSEipgXe43"
      },
      "outputs": [],
      "source": [
        "# DIN 的实现\n",
        "# -*- coding:utf-8 -*-\n",
        "\"\"\"\n",
        "Author:\n",
        "    Weichen Shen, weichenswc@163.com\n",
        "\n",
        "Reference:\n",
        "    [1] Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068. (https://arxiv.org/pdf/1706.06978.pdf)\n",
        "\"\"\"\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "#from ...feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, build_input_features\n",
        "#from ...inputs import create_embedding_matrix, embedding_lookup, get_dense_input, varlen_embedding_lookup, \\\n",
        "#    get_varlen_pooling_list\n",
        "#from ...layers.core import DNN, PredictionLayer\n",
        "#from ...layers.sequence import AttentionSequencePoolingLayer\n",
        "#from ...layers.utils import concat_func, combined_dnn_input\n",
        "\n",
        "\n",
        "def DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False,\n",
        "        dnn_hidden_units=(256, 128, 64), dnn_activation='relu', att_hidden_size=(80, 40), att_activation=\"dice\",\n",
        "        att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, seed=1024,\n",
        "        task='binary'):\n",
        "    \"\"\"\n",
        "    实现 Deep Interest Network（DIN）架构。\n",
        "\n",
        "    DIN 模型结合了深度神经网络（DNN）和注意力机制（Attention），用于处理序列推荐问题。\n",
        "    模型通过学习用户的历史行为序列，捕捉用户的兴趣，并预测用户对特定项目的兴趣。\n",
        "\n",
        "    参数：\n",
        "    dnn_feature_columns (list): 用于模型深度部分的所有特征列。\n",
        "    history_feature_list (list): 表示序列稀疏字段的列表。\n",
        "    dnn_use_bn (bool): 是否在深度网络中的激活函数前使用批量归一化。默认为 False。\n",
        "    dnn_hidden_units (tuple): 深度网络中每层的单元数。默认为 (256, 128, 64)。\n",
        "    dnn_activation (str): 深度网络中使用的激活函数。默认为 'relu'。\n",
        "    att_hidden_size (tuple): 注意力网络中每层的单元数。默认为 (80, 40)。\n",
        "    att_activation (str): 注意力网络中使用的激活函数。默认为 \"dice\"。\n",
        "    att_weight_normalization (bool): 是否对局部激活单元的注意力分数进行归一化。默认为 False。\n",
        "    l2_reg_dnn (float): 应用于 DNN 的 L2 正则化强度。默认为 0。\n",
        "    l2_reg_embedding (float): 应用于嵌入向量的 L2 正则化强度。默认为 1e-6。\n",
        "    dnn_dropout (float): DNN 中的dropout概率。默认为 0。\n",
        "    seed (int): 随机种子。默认为 1024。\n",
        "    task (str): 任务类型，'binary' 表示二分类逻辑回归，'regression' 表示回归任务。默认为 'binary'。\n",
        "\n",
        "    返回：\n",
        "    Keras 模型实例。\n",
        "    \"\"\"\n",
        "\n",
        "    features = build_input_features(dnn_feature_columns)\n",
        "\n",
        "    sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
        "    dense_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
        "\n",
        "    history_feature_columns = []\n",
        "    sparse_varlen_feature_columns = []\n",
        "    history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n",
        "    for fc in varlen_sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        if feature_name in history_fc_names:\n",
        "            history_feature_columns.append(fc)\n",
        "        else:\n",
        "            sparse_varlen_feature_columns.append(fc)\n",
        "\n",
        "    inputs_list = list(features.values())\n",
        "\n",
        "    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, seed, prefix=\"\")\n",
        "\n",
        "    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns, history_feature_list,\n",
        "                                      history_feature_list, to_list=True)\n",
        "    keys_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns, history_fc_names,\n",
        "                                     history_fc_names, to_list=True)\n",
        "    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,\n",
        "                                          mask_feat_list=history_feature_list, to_list=True)\n",
        "    dense_value_list = get_dense_input(features, dense_feature_columns)\n",
        "\n",
        "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict, features, sparse_varlen_feature_columns)\n",
        "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,\n",
        "                                                  to_list=True)\n",
        "\n",
        "    dnn_input_emb_list += sequence_embed_list\n",
        "\n",
        "    keys_emb = concat_func(keys_emb_list, mask=True)\n",
        "\n",
        "    deep_input_emb = concat_func(dnn_input_emb_list)\n",
        "    query_emb = concat_func(query_emb_list, mask=True)\n",
        "    hist = AttentionSequencePoolingLayer(att_hidden_size, att_activation,\n",
        "                        weight_normalization=att_weight_normalization,\n",
        "                        supports_masking=True)([query_emb, keys_emb])\n",
        "\n",
        "    #zjh_test\n",
        "    print(\"deep_input_emb\",deep_input_emb)\n",
        "    print(\"**********************************\")\n",
        "    print(\"hist\",hist)\n",
        "    deep_input_emb = concat_func([deep_input_emb, hist])#这里报错\n",
        "    deep_input_emb = Flatten()(deep_input_emb)\n",
        "    dnn_input = combined_dnn_input([deep_input_emb], dense_value_list)\n",
        "    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)\n",
        "    final_logit = Dense(1, use_bias=False)(output)\n",
        "\n",
        "    output = PredictionLayer(task)(final_logit)\n",
        "\n",
        "    model = Model(inputs=inputs_list, outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgR3ilBhaZvc"
      },
      "outputs": [],
      "source": [
        "# 导入必要的包\n",
        "import numpy as np\n",
        "\n",
        "# from deepctr.models import DIN\n",
        "\n",
        "# from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9cQ0D-cXrAW"
      },
      "outputs": [],
      "source": [
        "# SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQEtMZR2X7Sd"
      },
      "outputs": [],
      "source": [
        "def get_xy_fd():\n",
        "    feature_columns = [\n",
        "        SparseFeat('user', 3, embedding_dim=10), ## 用户ID，3个不同值，嵌入维度为10\n",
        "        SparseFeat('gender', 2, embedding_dim=4),##  # 性别，2个不同值，嵌入维度为4\n",
        "        SparseFeat('item_id', 4, embedding_dim=8),##  # 商品ID，4个不同值，嵌入维度为8\n",
        "        SparseFeat('cate_id', 3, embedding_dim=4),##  # 类别ID，3个不同值，嵌入维度为4\n",
        "        DenseFeat('pay_score', 1)##  # 支付评分，连续值，1维\n",
        "    ]\n",
        "    # # 增加变长稀疏特征列\n",
        "    feature_columns += [\n",
        "        # # 历史商品ID，最多4个历史记录，嵌入维度为8，嵌入名称为'item_id'\n",
        "        VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=4, embedding_dim=8, embedding_name='item_id'), maxlen=4, length_name=\"seq_length\"),\n",
        "        # # 历史类别ID，最多4个历史记录，嵌入维度为4，嵌入名称为'cate_id'\n",
        "        VarLenSparseFeat(SparseFeat('hist_cate_id', 3, embedding_dim=4, embedding_name='cate_id'), maxlen=4, length_name=\"seq_length\")\n",
        "    ]\n",
        "    # # 定义行为特征列表\n",
        "    behavior_feature_list = [\"item_id\", \"cate_id\"]\n",
        "    uid = np.array([0, 1, 2])## 用户ID\n",
        "    ugender = np.array([0, 1, 0])# 性别\n",
        "    iid = np.array([1, 2, 3])#商品id\n",
        "    cate_id = np.array([1, 2, 2])#类别id\n",
        "    pay_score = np.array([0.1, 0.2, 0.3])##支付评分\n",
        "    hist_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [1, 2, 0, 0]])##历史商品id\n",
        "    hist_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [1, 2, 0, 0]])# # 历史类别ID\n",
        "    seq_length = np.array([3, 3, 2])#序列长度\n",
        "    ## 创建特征字典\n",
        "    feature_dict = {'user': uid, 'gender': ugender, 'item_id': iid, 'cate_id': cate_id, 'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id, 'pay_score': pay_score, 'seq_length': seq_length}\n",
        "    # # 获取模型输入\n",
        "    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}\n",
        "    ## 目标变量\n",
        "    y = np.array([1, 0, 1])\n",
        "    # # 返回特征、目标和特征列\n",
        "    return x, y, feature_columns, behavior_feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFayOK5-YGpg",
        "outputId": "d1d5df78-712f-4ec8-d633-1be5907481ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deep_input_emb KerasTensor(type_spec=TensorSpec(shape=(None, 1, 26), dtype=tf.float32, name=None), name='concat_1/concat:0', description=\"created by layer 'concat_1'\")\n",
            "**********************************\n",
            "hist KerasTensor(type_spec=TensorSpec(shape=(None, 1, 12), dtype=tf.float32, name=None), name='attention_sequence_pooling_layer/MatMul:0', description=\"created by layer 'attention_sequence_pooling_layer'\")\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6891 - binary_crossentropy: 0.6891 - val_loss: 0.6894 - val_binary_crossentropy: 0.6894\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6781 - binary_crossentropy: 0.6781 - val_loss: 0.6883 - val_binary_crossentropy: 0.6883\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6688 - binary_crossentropy: 0.6688 - val_loss: 0.6876 - val_binary_crossentropy: 0.6876\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6592 - binary_crossentropy: 0.6592 - val_loss: 0.6868 - val_binary_crossentropy: 0.6868\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6485 - binary_crossentropy: 0.6485 - val_loss: 0.6867 - val_binary_crossentropy: 0.6867\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6369 - binary_crossentropy: 0.6369 - val_loss: 0.6868 - val_binary_crossentropy: 0.6868\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6245 - binary_crossentropy: 0.6245 - val_loss: 0.6870 - val_binary_crossentropy: 0.6870\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6110 - binary_crossentropy: 0.6110 - val_loss: 0.6876 - val_binary_crossentropy: 0.6876\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5963 - binary_crossentropy: 0.5963 - val_loss: 0.6888 - val_binary_crossentropy: 0.6888\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5803 - binary_crossentropy: 0.5803 - val_loss: 0.6906 - val_binary_crossentropy: 0.6906\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    x, y, feature_columns, behavior_feature_list = get_xy_fd()\n",
        "    model = DIN(feature_columns, behavior_feature_list)\n",
        "    model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy'])\n",
        "    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "V3BsBMe8USS3",
        "outputId": "1aea3080-9f8e-4c59-f095-f83258e7ebcc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.keras.callbacks.History</b><br/>def __init__()</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/callbacks.py</a>Callback that records events into a `History` object.\n",
              "\n",
              "This callback is automatically applied to\n",
              "every Keras model. The `History` object\n",
              "gets returned by the `fit` method of models.\n",
              "\n",
              "Example:\n",
              "\n",
              "&gt;&gt;&gt; model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
              "&gt;&gt;&gt; model.compile(tf.keras.optimizers.SGD(), loss=&#x27;mse&#x27;)\n",
              "&gt;&gt;&gt; history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
              "...                     epochs=10, verbose=1)\n",
              "&gt;&gt;&gt; print(history.params)\n",
              "{&#x27;verbose&#x27;: 1, &#x27;epochs&#x27;: 10, &#x27;steps&#x27;: 1}\n",
              "&gt;&gt;&gt; # check the keys of history object\n",
              "&gt;&gt;&gt; print(history.history.keys())\n",
              "dict_keys([&#x27;loss&#x27;])</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1139);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "tensorflow.python.keras.callbacks.History"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvXe-PxsYe0d",
        "outputId": "b024203d-d604-4515-ec5d-360452dff2d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'user': array([0, 1, 2]), 'gender': array([0, 1, 0]), 'item_id': array([1, 2, 3]), 'cate_id': array([1, 2, 2]), 'pay_score': array([0.1, 0.2, 0.3]), 'hist_item_id': array([[1, 2, 3, 0],\n",
            "       [3, 2, 1, 0],\n",
            "       [1, 2, 0, 0]]), 'seq_length': array([3, 3, 2]), 'hist_cate_id': array([[1, 2, 2, 0],\n",
            "       [2, 2, 1, 0],\n",
            "       [1, 2, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJUtPSWzX_Gt"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_new)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
