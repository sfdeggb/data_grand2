{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install deepctr[cpu]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ER3OcerVvU4D",
        "outputId": "e6e62947-d6f4-4532-8cb2-ce375b326c29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepctr[cpu] in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepctr[cpu]) (2.32.3)\n",
            "Requirement already satisfied: h5py==3.7.0 in /usr/local/lib/python3.10/dist-packages (from deepctr[cpu]) (3.7.0)\n",
            "Requirement already satisfied: tensorflow!=1.7.*,!=1.8.*,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from deepctr[cpu]) (2.15.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.7.0->deepctr[cpu]) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (2.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr[cpu]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr[cpu]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr[cpu]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr[cpu]) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow!=1.7.*,!=1.8.*,>=1.4.0->deepctr[cpu]) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow=='2.8.4'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "c2qMjuUww7gB",
        "outputId": "78bdadaa-074d-4a4b-8909-104946f11cef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8.4\n",
            "  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (3.7.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.4)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (3.4.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.8.4)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.4)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8 (from tensorflow==2.8.4)\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.4)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.4) (1.68.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.4) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.4) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.4)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.4) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.4) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.4)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.4)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.4) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.4) (3.2.2)\n",
            "Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-estimator, tensorboard-plugin-wit, keras, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.1\n",
            "    Uninstalling tensorflow-2.15.1:\n",
            "      Successfully uninstalled tensorflow-2.15.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.73.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.4 tensorflow-estimator-2.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "799543130a924dcc86d2080516b6b361"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepctr.feature_column import SparseFeat, DenseFeat,VarLenSparseFeat\n",
        "\n",
        "def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,\n",
        "                               support_dense=True, support_group=False):\n",
        "    sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
        "\n",
        "    embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix,\n",
        "                                                    seq_mask_zero=seq_mask_zero)\n",
        "    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n",
        "    dense_value_list = get_dense_input(features, feature_columns)\n",
        "    if not support_dense and len(dense_value_list) > 0:\n",
        "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
        "\n",
        "    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n",
        "    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n",
        "                                                                 varlen_sparse_feature_columns)\n",
        "    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n",
        "    if not support_group:\n",
        "        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n",
        "    return group_embedding_dict, dense_value_list"
      ],
      "metadata": {
        "id": "USL58H0hsCdf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, OrderedDict\n",
        "from tensorflow.python.keras.layers import Input, Lambda\n",
        "\n",
        "def build_input_features(feature_columns, prefix=''):\n",
        "    input_features = OrderedDict()\n",
        "    for fc in feature_columns:\n",
        "        if isinstance(fc, SparseFeat):\n",
        "            input_features[fc.name] = Input(\n",
        "                shape=(1,), name=prefix + fc.name, dtype=fc.dtype)\n",
        "        elif isinstance(fc, DenseFeat):\n",
        "            input_features[fc.name] = Input(\n",
        "                shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
        "        elif isinstance(fc, VarLenSparseFeat):\n",
        "            input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + fc.name,\n",
        "                                            dtype=fc.dtype)\n",
        "            if fc.weight_name is not None:\n",
        "                input_features[fc.weight_name] = Input(shape=(fc.maxlen, 1), name=prefix + fc.weight_name,\n",
        "                                                       dtype=\"float32\")\n",
        "            if fc.length_name is not None:\n",
        "                input_features[fc.length_name] = Input((1,), name=prefix + fc.length_name, dtype='int32')\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Invalid feature column type,got\", type(fc))\n",
        "\n",
        "    return input_features"
      ],
      "metadata": {
        "id": "hAKcDirRqKSP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.ops.init_ops import Zeros\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.init_ops_v2 import Zeros\n",
        "from tensorflow.python.keras.layers import Layer, Activation\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.keras.layers import BatchNormalization\n",
        "except ImportError:\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "\n",
        "try:\n",
        "   unicode\n",
        "except NameError:\n",
        "    unicode = str\n",
        "\n",
        "class Dice(Layer):\n",
        "    \"\"\"The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.\n",
        "\n",
        "      Input shape\n",
        "        - Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "      Output shape\n",
        "        - Same shape as the input.\n",
        "\n",
        "      Arguments\n",
        "        - **axis** : Integer, the axis that should be used to compute data distribution (typically the features axis).\n",
        "\n",
        "        - **epsilon** : Small float added to variance to avoid dividing by zero.\n",
        "\n",
        "      References\n",
        "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis=-1, epsilon=1e-9, **kwargs):\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        super(Dice, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bn = BatchNormalization(\n",
        "            axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
        "        self.alphas = self.add_weight(shape=(input_shape[-1],), initializer=Zeros(\n",
        "        ), dtype=tf.float32, name='dice_alpha')  # name='alpha_'+self.name\n",
        "        super(Dice, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "        self.uses_learning_phase = True\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        inputs_normed = self.bn(inputs, training=training)\n",
        "        # tf.layers.batch_normalization(\n",
        "        # inputs, axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
        "        x_p = tf.sigmoid(inputs_normed)\n",
        "        return self.alphas * (1.0 - x_p) * inputs + x_p * inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'axis': self.axis, 'epsilon': self.epsilon}\n",
        "        base_config = super(Dice, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def activation_layer(activation):\n",
        "    if activation in (\"dice\", \"Dice\"):\n",
        "        act_layer = Dice()\n",
        "    elif isinstance(activation, (str, unicode)):\n",
        "        act_layer = Activation(activation)\n",
        "    elif issubclass(activation, Layer):\n",
        "        act_layer = activation()\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid activation,found %s.You should use a str or a Activation Layer Class.\" % (activation))\n",
        "    return act_layer"
      ],
      "metadata": {
        "id": "kKLsmz2SuTkX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from tensorflow.python.keras.layers import BatchNormalization\n",
        "except ImportError:\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "\n",
        "# from .activation import activation_layer\n",
        "\n",
        "class DNN(Layer):\n",
        "    \"\"\"The Multi Layer Percetron\n",
        "\n",
        "      Input shape\n",
        "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
        "\n",
        "      Output shape\n",
        "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
        "\n",
        "      Arguments\n",
        "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
        "\n",
        "        - **activation**: Activation function to use.\n",
        "\n",
        "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
        "\n",
        "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
        "\n",
        "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
        "\n",
        "        - **output_activation**: Activation function to use in the last layer.If ``None``,it will be same as ``activation``.\n",
        "\n",
        "        - **seed**: A Python integer to use as random seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, output_activation=None,\n",
        "                 seed=1024, **kwargs):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.l2_reg = l2_reg\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_bn = use_bn\n",
        "        self.output_activation = output_activation\n",
        "        self.seed = seed\n",
        "\n",
        "        super(DNN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # if len(self.hidden_units) == 0:\n",
        "        #     raise ValueError(\"hidden_units is empty\")\n",
        "        input_size = input_shape[-1]\n",
        "        hidden_units = [int(input_size)] + list(self.hidden_units)\n",
        "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
        "                                        shape=(\n",
        "                                            hidden_units[i], hidden_units[i + 1]),\n",
        "                                        initializer=glorot_normal(\n",
        "                                            seed=self.seed),\n",
        "                                        regularizer=l2(self.l2_reg),\n",
        "                                        trainable=True) for i in range(len(self.hidden_units))]\n",
        "        self.bias = [self.add_weight(name='bias' + str(i),\n",
        "                                     shape=(self.hidden_units[i],),\n",
        "                                     initializer=Zeros(),\n",
        "                                     trainable=True) for i in range(len(self.hidden_units))]\n",
        "        if self.use_bn:\n",
        "            self.bn_layers = [BatchNormalization() for _ in range(len(self.hidden_units))]\n",
        "\n",
        "        self.dropout_layers = [Dropout(self.dropout_rate, seed=self.seed + i) for i in\n",
        "                               range(len(self.hidden_units))]\n",
        "\n",
        "        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]\n",
        "\n",
        "        if self.output_activation:\n",
        "            self.activation_layers[-1] = activation_layer(self.output_activation)\n",
        "\n",
        "        super(DNN, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "\n",
        "        deep_input = inputs\n",
        "\n",
        "        for i in range(len(self.hidden_units)):\n",
        "            fc = tf.nn.bias_add(tf.tensordot(\n",
        "                deep_input, self.kernels[i], axes=(-1, 0)), self.bias[i])\n",
        "\n",
        "            if self.use_bn:\n",
        "                fc = self.bn_layers[i](fc, training=training)\n",
        "            try:\n",
        "                fc = self.activation_layers[i](fc, training=training)\n",
        "            except TypeError as e:  # TypeError: call() got an unexpected keyword argument 'training'\n",
        "                print(\"make sure the activation function use training flag properly\", e)\n",
        "                fc = self.activation_layers[i](fc)\n",
        "\n",
        "            fc = self.dropout_layers[i](fc, training=training)\n",
        "            deep_input = fc\n",
        "\n",
        "        return deep_input\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if len(self.hidden_units) > 0:\n",
        "            shape = input_shape[:-1] + (self.hidden_units[-1],)\n",
        "        else:\n",
        "            shape = input_shape\n",
        "\n",
        "        return tuple(shape)\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
        "                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate,\n",
        "                  'output_activation': self.output_activation, 'seed': self.seed}\n",
        "        base_config = super(DNN, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class PredictionLayer(Layer):\n",
        "    \"\"\"\n",
        "      Arguments\n",
        "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
        "\n",
        "         - **use_bias**: bool.Whether add bias term or not.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
        "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
        "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
        "        self.task = task\n",
        "        self.use_bias = use_bias\n",
        "        super(PredictionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.global_bias = self.add_weight(\n",
        "                shape=(1,), initializer=Zeros(), name=\"global_bias\")\n",
        "\n",
        "        # Be sure to call this somewhere!\n",
        "        super(PredictionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        if self.use_bias:\n",
        "            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')\n",
        "        if self.task == \"binary\":\n",
        "            x = tf.sigmoid(x)\n",
        "\n",
        "        output = tf.reshape(x, (-1, 1))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1)\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'task': self.task, 'use_bias': self.use_bias}\n",
        "        base_config = super(PredictionLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "metadata": {
        "id": "8SYiBr2AsNMk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_sum(input_tensor,\n",
        "               axis=None,\n",
        "               keep_dims=False,\n",
        "               name=None,\n",
        "               reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor,\n",
        "                             axis=axis,\n",
        "                             keep_dims=keep_dims,\n",
        "                             name=name,\n",
        "                             reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_sum(input_tensor,\n",
        "                             axis=axis,\n",
        "                             keepdims=keep_dims,\n",
        "                             name=name)"
      ],
      "metadata": {
        "id": "balxZx7ksmrm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.layers import Flatten, Layer, Add\n",
        "\n",
        "# -*- coding:utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "Author:\n",
        "    Weichen Shen,weichenswc@163.com\n",
        "\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import Flatten, Layer, Add\n",
        "from tensorflow.python.ops.lookup_ops import TextFileInitializer\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.ops.init_ops import Zeros, glorot_normal_initializer as glorot_normal\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.init_ops_v2 import Zeros, glorot_normal\n",
        "\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.ops.lookup_ops import StaticHashTable\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.lookup_ops import HashTable as StaticHashTable\n",
        "\n",
        "class NoMask(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NoMask, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Be sure to call this somewhere!\n",
        "        super(NoMask, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None, **kwargs):\n",
        "        return x\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "\n",
        "class Concat(Layer):\n",
        "    def __init__(self, axis, supports_masking=True, **kwargs):\n",
        "        super(Concat, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "        self.supports_masking = supports_masking\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.concat(inputs, axis=self.axis)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if not self.supports_masking:\n",
        "            return None\n",
        "        if mask is None:\n",
        "            mask = [inputs_i._keras_mask if hasattr(inputs_i, \"_keras_mask\") else None for inputs_i in inputs]\n",
        "        if mask is None:\n",
        "            return None\n",
        "        if not isinstance(mask, list):\n",
        "            raise ValueError('`mask` should be a list.')\n",
        "        if not isinstance(inputs, list):\n",
        "            raise ValueError('`inputs` should be a list.')\n",
        "        if len(mask) != len(inputs):\n",
        "            raise ValueError('The lists `inputs` and `mask` '\n",
        "                             'should have the same length.')\n",
        "        if all([m is None for m in mask]):\n",
        "            return None\n",
        "        # Make a list of masks while making sure\n",
        "        # the dimensionality of each mask\n",
        "        # is the same as the corresponding input.\n",
        "        masks = []\n",
        "        for input_i, mask_i in zip(inputs, mask):\n",
        "            if mask_i is None:\n",
        "                # Input is unmasked. Append all 1s to masks,\n",
        "                masks.append(tf.ones_like(input_i, dtype='bool'))\n",
        "            elif K.ndim(mask_i) < K.ndim(input_i):\n",
        "                # Mask is smaller than the input, expand it\n",
        "                masks.append(tf.expand_dims(mask_i, axis=-1))\n",
        "            else:\n",
        "                masks.append(mask_i)\n",
        "        concatenated = K.concatenate(masks, axis=self.axis)\n",
        "        return K.all(concatenated, axis=-1, keepdims=False)\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'axis': self.axis, 'supports_masking': self.supports_masking}\n",
        "        base_config = super(Concat, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def concat_func(inputs, axis=-1, mask=False):\n",
        "    if len(inputs) == 1:\n",
        "        input = inputs[0]\n",
        "        if not mask:\n",
        "            input = NoMask()(input)\n",
        "        return input\n",
        "    return Concat(axis, supports_masking=mask)(inputs)\n",
        "\n",
        "def combined_dnn_input(sparse_embedding_list, dense_value_list):\n",
        "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
        "        sparse_dnn_input = Flatten()(concat_func(sparse_embedding_list))\n",
        "        dense_dnn_input = Flatten()(concat_func(dense_value_list))\n",
        "        return concat_func([sparse_dnn_input, dense_dnn_input])\n",
        "    elif len(sparse_embedding_list) > 0:\n",
        "        return Flatten()(concat_func(sparse_embedding_list))\n",
        "    elif len(dense_value_list) > 0:\n",
        "        return Flatten()(concat_func(dense_value_list))\n",
        "    else:\n",
        "        raise NotImplementedError(\"dnn_feature_columns can not be empty list\")"
      ],
      "metadata": {
        "id": "GNtrELbDsq2q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tG7G0R_9pZOs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Dense, Lambda\n",
        "\n",
        "#from ...feature_column import build_input_features, input_from_feature_columns\n",
        "#from ...layers.core import PredictionLayer, DNN\n",
        "#from ...layers.utils import combined_dnn_input, reduce_sum\n",
        "\n",
        "\n",
        "def MMOE(dnn_feature_columns, num_experts=3,expert_dnn_hidden_units=(256, 128),tower_dnn_hidden_units=(64,),gate_dnn_hidden_units=(),l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,\n",
        "         dnn_activation='relu',\n",
        "         dnn_use_bn=False, task_types=('binary', 'binary'), task_names=('ctr', 'ctcvr')):\n",
        "    \"\"\"Instantiates the Multi-gate Mixture-of-Experts multi-task learning architecture.\n",
        "\n",
        "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
        "    :param num_experts: integer, number of experts.\n",
        "    :param expert_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of expert DNN.\n",
        "    :param tower_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN.\n",
        "    :param gate_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of gate DNN.\n",
        "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
        "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
        "    :param seed: integer ,to use as random seed.\n",
        "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
        "    :param dnn_activation: Activation function to use in DNN\n",
        "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
        "    :param task_types: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n",
        "    :param task_names: list of str, indicating the predict target of each tasks\n",
        "\n",
        "    :return: a Keras model instance\n",
        "    \"\"\"\n",
        "    num_tasks = len(task_names)\n",
        "    if num_tasks <= 1:\n",
        "        raise ValueError(\"num_tasks must be greater than 1\")\n",
        "    if num_experts <= 1:\n",
        "        raise ValueError(\"num_experts must be greater than 1\")\n",
        "\n",
        "    if len(task_types) != num_tasks:\n",
        "        raise ValueError(\"num_tasks must be equal to the length of task_types\")\n",
        "\n",
        "    for task_type in task_types:\n",
        "        if task_type not in ['binary', 'regression']:\n",
        "            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task_type))\n",
        "\n",
        "    features = build_input_features(dnn_feature_columns)\n",
        "\n",
        "    inputs_list = list(features.values())\n",
        "\n",
        "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
        "                                                                         l2_reg_embedding, seed)\n",
        "    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
        "\n",
        "    # build expert layer\n",
        "    expert_outs = []\n",
        "    for i in range(num_experts):\n",
        "        expert_network = DNN(expert_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,\n",
        "                             name='expert_' + str(i))(dnn_input)\n",
        "        expert_outs.append(expert_network)\n",
        "\n",
        "    expert_concat = Lambda(lambda x: tf.stack(x, axis=1))(expert_outs)  # None,num_experts,dim\n",
        "\n",
        "    mmoe_outs = []\n",
        "    for i in range(num_tasks):  # one mmoe layer: nums_tasks = num_gates\n",
        "        # build gate layers\n",
        "        gate_input = DNN(gate_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,\n",
        "                         name='gate_' + task_names[i])(dnn_input)\n",
        "        gate_out = Dense(num_experts, use_bias=False, activation='softmax',\n",
        "                         name='gate_softmax_' + task_names[i])(gate_input)\n",
        "        gate_out = Lambda(lambda x: tf.expand_dims(x, axis=-1))(gate_out)\n",
        "\n",
        "        # gate multiply the expert\n",
        "        gate_mul_expert = Lambda(lambda x: reduce_sum(x[0] * x[1], axis=1, keep_dims=False),\n",
        "                                 name='gate_mul_expert_' + task_names[i])([expert_concat, gate_out])\n",
        "        mmoe_outs.append(gate_mul_expert)\n",
        "\n",
        "    task_outs = []\n",
        "    for task_type, task_name, mmoe_out in zip(task_types, task_names, mmoe_outs):\n",
        "        # build tower layer\n",
        "        tower_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,\n",
        "                           name='tower_' + task_name)(mmoe_out)\n",
        "\n",
        "        logit = Dense(1, use_bias=False)(tower_output)\n",
        "        output = PredictionLayer(task_type, name=task_name)(logit)\n",
        "        task_outs.append(output)\n",
        "\n",
        "    model = Model(inputs=inputs_list, outputs=task_outs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from deepctr.models import MMOE\n",
        "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
        "\n",
        "# 1. 数据加载和预处理\n",
        "def data_preprocessing(data):\n",
        "    # 类别特征编码\n",
        "    sparse_features = ['gender', 'region', 'user_hot', 'video_type', 'video_quality']\n",
        "    dense_features = ['age', 'video_length', 'stay_time']\n",
        "\n",
        "    # 标签编码\n",
        "    for feat in sparse_features:\n",
        "        lbe = LabelEncoder()\n",
        "        data[feat] = lbe.fit_transform(data[feat])\n",
        "\n",
        "    # 数值特征归一化\n",
        "    mms = MinMaxScaler(feature_range=(0, 1))\n",
        "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
        "\n",
        "    # 构建特征列\n",
        "    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=4)\n",
        "                            for feat in sparse_features] + [DenseFeat(feat, 1) for feat in dense_features]\n",
        "\n",
        "    dnn_feature_columns = fixlen_feature_columns\n",
        "    linear_feature_columns = fixlen_feature_columns\n",
        "\n",
        "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
        "\n",
        "    return data, feature_names, linear_feature_columns, dnn_feature_columns\n",
        "\n",
        "# 2. 主函数\n",
        "def main():\n",
        "    # 加载数据\n",
        "    data = pd.read_csv('/content/mulit_task_data.csv')\n",
        "\n",
        "    # 数据预处理\n",
        "    data, feature_names, linear_feature_columns, dnn_feature_columns = data_preprocessing(data)\n",
        "\n",
        "    # 准备训练数据\n",
        "    train, test = train_test_split(data, test_size=0.2, random_state=2024)\n",
        "\n",
        "    train_model_input = {name: train[name] for name in feature_names}\n",
        "    test_model_input = {name: test[name] for name in feature_names}\n",
        "\n",
        "    # 构建MMOE模型\n",
        "    model = MMOE(dnn_feature_columns,\n",
        "                 linear_feature_columns,\n",
        "                 task_types=['binary', 'binary'],\n",
        "                 task_names=['watch', 'buy'],\n",
        "                 #expert_dim=8,\n",
        "                 num_experts=8,\n",
        "                 mmoe_hidden_units=(128, 64),\n",
        "                 dnn_hidden_units=(64, 32))\n",
        "\n",
        "    # 模型编译\n",
        "    model.compile(\"adam\",\n",
        "                 loss=[\"binary_crossentropy\", \"binary_crossentropy\"],\n",
        "                 metrics=['binary_crossentropy', 'AUC'],\n",
        "                 loss_weights=[1.0, 1.0])\n",
        "\n",
        "    # 模型训练\n",
        "    history = model.fit(train_model_input,\n",
        "                       [train['is_watch'].values, train['is_buy'].values],\n",
        "                       batch_size=256,\n",
        "                       epochs=10,\n",
        "                       verbose=2,\n",
        "                       validation_split=0.2)\n",
        "\n",
        "    # 模型评估\n",
        "    print(\"测试集评估结果:\")\n",
        "    model.evaluate(test_model_input,\n",
        "                  [test['is_watch'].values, test['is_buy'].values])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "collapsed": true,
        "id": "3ka0ORwWrGU_",
        "outputId": "972bd503-3289-405a-e204-98175e6e4827"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MMOE() got multiple values for argument 'num_experts'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d696a87db8b8>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-d696a87db8b8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# 构建MMOE模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     model = MMOE(dnn_feature_columns, \n\u001b[0m\u001b[1;32m     50\u001b[0m                  \u001b[0mlinear_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                  \u001b[0mtask_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MMOE() got multiple values for argument 'num_experts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A16iGJdqyDsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}